{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy\n",
    "import sense2vec\n",
    "import random as rn\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "nlp = spacy.load('en')\n",
    "# model = sense2vec.load()\n",
    "\n",
    "from data_handling_for_heuristic import *\n",
    "#read_path = 'data/conll2003/eng.train'\n",
    "read_path = 'train.txt'\n",
    "raw_data, label_data = load_conll2003(read_path)\n",
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "엔티티-교체는 단어-교체와 다르게, 엔티티가 있는 모든 데이터를 대상으로 한다. <br>\n",
    "완전한 문장일 필요가 구지 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOC_data = load('entity_info/LOC_data')\n",
    "PER_data = load('entity_info/PER_data')\n",
    "ORG_data = load('entity_info/ORG_data')\n",
    "# LOC_sense2vec = load('entity_info/LOC_sense2vec')\n",
    "# PER_sense2vec = load('entity_info/PER_sense2vec')\n",
    "# ORG_sense2vec = load('entity_info/ORG_sense2vec')\n",
    "\n",
    "# LOC_sense2vec_unique = load('entity_info/LOC_sense2vec_unique')\n",
    "# PER_sense2vec_unique = load('entity_info/PER_sense2vec_unique')\n",
    "# ORG_sense2vec_unique = load('entity_info/ORG_sense2vec_unique')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## for sense2vec_unique\n",
    "# LOC_sense2vec = {}\n",
    "# for pair in LOC_sense2vec_unique:\n",
    "#     source = pair[0]\n",
    "#     target = pair[1]\n",
    "#     LOC_sense2vec[source] = target\n",
    "\n",
    "# PER_sense2vec = {}\n",
    "# for pair in PER_sense2vec_unique:\n",
    "#     source = pair[0]\n",
    "#     target = pair[1]\n",
    "#     PER_sense2vec[source] = target\n",
    "    \n",
    "# ORG_sense2vec = {}\n",
    "# for pair in ORG_sense2vec_unique:\n",
    "#     source = pair[0]\n",
    "#     target = pair[1]\n",
    "#     ORG_sense2vec[source] = target\n",
    "\n",
    "############ sense2vec 쓸 때 사용\n",
    "#################################################\n",
    "# LOC_sense2vec_dic = {}\n",
    "# for pair in LOC_sense2vec:\n",
    "        \n",
    "#     source = pair[0]\n",
    "#     target = pair[1]    \n",
    "    \n",
    "#     try:\n",
    "#         LOC_sense2vec_dic[source]\n",
    "#         LOC_sense2vec_dic[source].append(target)\n",
    "#     except(KeyError):\n",
    "#         LOC_sense2vec_dic[source] = [target]\n",
    "        \n",
    "# PER_sense2vec_dic = {}\n",
    "# for pair in LOC_sense2vec:\n",
    "        \n",
    "#     source = pair[0]\n",
    "#     target = pair[1]    \n",
    "    \n",
    "#     try:\n",
    "#         PER_sense2vec_dic[source]\n",
    "#         PER_sense2vec_dic[source].append(target)\n",
    "#     except(KeyError):\n",
    "#         PER_sense2vec_dic[source] = [target]\n",
    "        \n",
    "# ORG_sense2vec_dic = {}\n",
    "# for pair in LOC_sense2vec:\n",
    "        \n",
    "#     source = pair[0]\n",
    "#     target = pair[1]    \n",
    "    \n",
    "#     try:\n",
    "#         ORG_sense2vec_dic[source]\n",
    "#         ORG_sense2vec_dic[source].append(target)\n",
    "#     except(KeyError):\n",
    "#         ORG_sense2vec_dic[source] = [target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_label(input_entity, type_ent):\n",
    "    splited_entity = input_entity.split()\n",
    "    temp = ''\n",
    "    if len(splited_entity) == 1:\n",
    "        label = 'B-' + type_ent\n",
    "        return label\n",
    "    else:\n",
    "        total_label = ''\n",
    "        label = 'I-' + type_ent\n",
    "        for i in range(0, len(splited_entity)):\n",
    "            if i==0:\n",
    "                total_label += 'B-' + type_ent\n",
    "            else:\n",
    "                total_label += ' '\n",
    "                total_label += label\n",
    "        return total_label\n",
    "\n",
    "    \n",
    "############ sense2vec 쓸 때 사용\n",
    "#################################################    \n",
    "# def replace_entity(input_entity, type_ent):\n",
    "    \n",
    "#     try:\n",
    "#         if type_ent == 'LOC':\n",
    "#             cand_list = LOC_sense2vec_dic[input_entity]\n",
    "#             new_entity = cand_list[rn.randrange(0, len(cand_list))]\n",
    "#             new_label = extract_label(new_entity, type_ent)\n",
    "#         elif type_ent == 'PER':\n",
    "#             cand_list = PER_sense2vec_dic[input_entity]\n",
    "#             new_entity = cand_list[rn.randrange(0, len(cand_list))]\n",
    "#             new_label = extract_label(new_entity, type_ent)\n",
    "#         elif type_ent == 'ORG':\n",
    "#             cand_list = ORG_sense2vec_dic[input_entity]\n",
    "#             new_entity = cand_list[rn.randrange(0, len(cand_list))]\n",
    "#             new_label = extract_label(new_entity, type_ent)\n",
    "            \n",
    "#         return new_entity.split(), new_label.split()\n",
    "    \n",
    "#     except(KeyError):\n",
    "#         if type_ent == 'LOC':\n",
    "#             gen_num = (rn.randrange(1, len(LOC_data)))\n",
    "#             new_entity = LOC_data[gen_num]\n",
    "#             new_label = extract_label(new_entity, type_ent)\n",
    "            \n",
    "#         elif type_ent == 'PER':\n",
    "#             gen_num = (rn.randrange(1, len(PER_data)))\n",
    "#             new_entity = PER_data[gen_num]\n",
    "#             new_label = extract_label(new_entity, type_ent)\n",
    "            \n",
    "#         elif type_ent == 'ORG':    \n",
    "#             gen_num = (rn.randrange(1, len(ORG_data)))\n",
    "#             new_entity = ORG_data[gen_num]\n",
    "#             new_label = extract_label(new_entity, type_ent)\n",
    "            \n",
    "#         return new_entity.split(), new_label.split() # list형으로 변환.\n",
    "    \n",
    "    \n",
    "def replace_entity(input_entity, type_ent):\n",
    "\n",
    "    if type_ent == 'LOC':\n",
    "        gen_num = (rn.randrange(1, len(LOC_data)))\n",
    "        new_entity = LOC_data[gen_num]\n",
    "        new_label = extract_label(new_entity, type_ent)\n",
    "            \n",
    "    elif type_ent == 'PER':\n",
    "        gen_num = (rn.randrange(1, len(PER_data)))\n",
    "        new_entity = PER_data[gen_num]\n",
    "        new_label = extract_label(new_entity, type_ent)\n",
    "            \n",
    "    elif type_ent == 'ORG':    \n",
    "        gen_num = (rn.randrange(1, len(ORG_data)))\n",
    "        new_entity = ORG_data[gen_num]\n",
    "        new_label = extract_label(new_entity, type_ent)\n",
    "            \n",
    "    return new_entity.split(), new_label.split() # list형으로 변환.    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def is_only_MISC(label_list):\n",
    "    tkn = False \n",
    "    if 'B-MISC' in label_list:\n",
    "        if not 'B-PER' in label_list:\n",
    "            if not 'B-ORG' in label_list:\n",
    "                if not 'B-LOC' in label_list:\n",
    "                    tkn = True\n",
    "    return tkn \n",
    "\n",
    "def exist_ENTITY(label_list):\n",
    "    tkn = False\n",
    "    for token in label_list:\n",
    "        if token != 'O': # 엔티티가 하나라도 있으면.\n",
    "            tkn = True\n",
    "    return tkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10202\n"
     ]
    }
   ],
   "source": [
    "new_sent_raw_list = []\n",
    "new_sent_label_list = []\n",
    "\n",
    "for i, sent in enumerate(raw_data):\n",
    "    list_raw_data = raw_data[i].split()\n",
    "    list_label_data = label_data[i].split()\n",
    "    \n",
    "    # Initialization\n",
    "    temp_entity = ''\n",
    "    start_ent_tkn = -1\n",
    "    j = 0\n",
    "    new_sent_raw = ''\n",
    "    new_sent_label = ''\n",
    "\n",
    "    if is_only_MISC(list_label_data) == True:\n",
    "        continue # 문장에서 엔티티가 MISC만 있는 경우는 PASS\n",
    "    \n",
    "    if exist_ENTITY(list_label_data) == False:\n",
    "        continue # 문장에서 엔티티가 아예 없는 경우는 PASS\n",
    "    \n",
    "    #print(list_raw_data)\n",
    "    #print(list_label_data)    \n",
    "    #print('------------------------------------')\n",
    "    \n",
    "    while j < len(list_label_data):\n",
    "        type_ent = list_label_data[j].split('-')[-1]\n",
    "        #print(j, list_label_data[j], list_raw_data[j])\n",
    "        \n",
    "        if list_label_data[j] != 'O' and list_label_data[j] != 'B-MISC' and list_label_data[j] != 'I-MISC':\n",
    "            \n",
    "            if j == len(list_label_data)-1: # sent에서 마지막 token (따로 처리해줘야함.)\n",
    "                \n",
    "                if list_label_data[j].split('-')[0] == 'B':\n",
    "                    #\n",
    "                    start_ent_tkn = j\n",
    "                    \n",
    "                    ### 교체시작! (자기 혼자만.)\n",
    "                    temp_entity += list_raw_data[j]\n",
    "                    del list_raw_data[j]\n",
    "                    del list_label_data[j]                   \n",
    "                    \n",
    "                    new_entity, new_label = replace_entity(temp_entity, type_ent)\n",
    "                    new_sent_raw = list_raw_data[:j] + new_entity + list_raw_data[j:]\n",
    "                    new_sent_label = list_label_data[:j] + new_label + list_label_data[j:]\n",
    "                    \n",
    "                    list_raw_data = new_sent_raw\n",
    "                    list_label_data = new_sent_label                    \n",
    "                    \n",
    "                    j = j - 1 \n",
    "                    j += len(new_entity)\n",
    "\n",
    "                elif list_label_data[j].split('-')[0] == 'I':\n",
    "\n",
    "                    ### 교체시작! (history 합쳐서.)\n",
    "                    temp_entity += ' '\n",
    "                    temp_entity += list_raw_data[j]\n",
    "                    del list_raw_data[j]\n",
    "                    del list_label_data[j]                   \n",
    "                    \n",
    "                    #print(temp_entity)\n",
    "                    new_entity, new_label = replace_entity(temp_entity, type_ent)\n",
    "                    #print('new ->', new_entity, new_label)\n",
    "                    new_sent_raw = list_raw_data[:j] + new_entity + list_raw_data[j:]\n",
    "                    new_sent_label = list_label_data[:j] + new_label + list_label_data[j:]    \n",
    "                    \n",
    "                    list_raw_data = new_sent_raw\n",
    "                    list_label_data = new_sent_label \n",
    "                    \n",
    "                    j = j - 1 \n",
    "                    j += len(new_entity) \n",
    "\n",
    "            else: # All except for last one\n",
    "                if list_label_data[j].split('-')[0] == 'B':\n",
    "                    start_ent_tkn = j\n",
    "                    if list_label_data[j+1].split('-')[0] == 'I': \n",
    "                        ## 저장\n",
    "                        temp_entity += list_raw_data[j]\n",
    "                        del list_raw_data[j] # 현재 'B' 제거 (저장을 위해)\n",
    "                        del list_label_data[j] # 현재 'B' 제거 (저장을 위해)                   \n",
    "                        j = j - 1                       \n",
    "\n",
    "                    else: # 'B' 또는 'O\" 이라면..       \n",
    "                        ### 교체시작! (자기 혼자만.)\n",
    "                        temp_entity += list_raw_data[j]                        \n",
    "                        del list_raw_data[j] # 현재 'B' 제거 (교체를 위해)\n",
    "                        del list_label_data[j] # 현재 'B' 제거 (교체를 위해)         \n",
    "                        new_entity, new_label = replace_entity(temp_entity, type_ent)\n",
    "                        new_sent_raw = list_raw_data[:j] + new_entity + list_raw_data[j:]\n",
    "                        new_sent_label = list_label_data[:j] + new_label + list_label_data[j:]\n",
    "                        list_raw_data = new_sent_raw\n",
    "                        list_label_data = new_sent_label   \n",
    "                        j = j - 1 \n",
    "                        j += len(new_entity)\n",
    "                        temp_entity = '' # 초기화\n",
    "                \n",
    "                elif list_label_data[j].split('-')[0] == 'I':\n",
    "                    if list_label_data[j+1].split('-')[0] == 'I':  \n",
    "                        ## 저장\n",
    "                        temp_entity += ' '\n",
    "                        temp_entity += list_raw_data[j]                        \n",
    "                        del list_raw_data[j]\n",
    "                        del list_label_data[j]            \n",
    "                        j = j - 1                         \n",
    "\n",
    "                    else: # 'B' 또는 'O\" 이라면..\n",
    "                        ### 교체시작! (history 합쳐서.)\n",
    "                        temp_entity += list_raw_data[j]\n",
    "                        del list_raw_data[j]\n",
    "                        del list_label_data[j]\n",
    "                                               \n",
    "                        new_entity, new_label = replace_entity(temp_entity, type_ent)\n",
    "                        new_sent_raw = list_raw_data[:j] + new_entity + list_raw_data[j:]\n",
    "                        new_sent_label = list_label_data[:j] + new_label + list_label_data[j:]\n",
    "                        \n",
    "                        list_raw_data = new_sent_raw\n",
    "                        list_label_data = new_sent_label\n",
    "                        \n",
    "                        j = j - 1 \n",
    "                        j += len(new_entity)\n",
    "                        temp_entity = '' # 초기화\n",
    "        j += 1\n",
    "\n",
    "    #print(list_raw_data)\n",
    "    #print(list_label_data)\n",
    "    \n",
    "    # add new sentence\n",
    "    new_sent_raw_list.append(list_raw_data)\n",
    "    new_sent_label_list.append(list_label_data)\n",
    "    \n",
    "    #print('\\n')\n",
    "    \n",
    "print(len(new_sent_raw_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_write = '20171002_entity.txt'\n",
    "\n",
    "with open(path_write, 'w', encoding='UTF-8') as txt:\n",
    "    for i, _ in enumerate(new_sent_raw_list):\n",
    "        splited_sent = new_sent_raw_list[i]\n",
    "        splited_label = new_sent_label_list[i]\n",
    "        for j, token in enumerate(splited_sent):\n",
    "            txt.write(splited_sent[j]+' '+'NNP'+' '+'B-NP'+' '+splited_label[j])\n",
    "            txt.write('\\n')\n",
    "        txt.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
