{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주의해야 될점은 word embedding의 종류를 잘 선택하는 것도 중요하다. 예를 들어 모델에서 glove를 사용한다면, 여기서도 glove를 사용하는 것이 맞지 않나 생각한다. (word embedding 종류에 따라 similairty 수치가 차이가 날 것이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [filtering_none_entity]: before = 14987 and after = 11132\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "\"\"\" PARAMETER INIT \"\"\"\n",
    "################################################################################\n",
    "\n",
    "read_file_path = '../data/conll2003/en/train.txt'\n",
    "filter_incomplete_sent = False\n",
    "window_size = 3\n",
    "filter_none_entity = True\n",
    "\n",
    "################################################################################\n",
    "\n",
    "import spacy\n",
    "import numpy\n",
    "#import sense2vec\n",
    "import random\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "nlp = spacy.load('en')\n",
    "#model = sense2vec.load()\n",
    "\n",
    "########################\n",
    "### Load and Prepare Dataset\n",
    "from data_handling_for_heuristic import *\n",
    "#read_path = 'data/conll2003/eng.train'\n",
    "#read_path = 'train.txt'\n",
    "raw_data, label_data = load_conll2003(read_file_path)\n",
    "\n",
    "\n",
    "# ########################\n",
    "# ### 완벽한 문장만 필터링\n",
    "# if filter_incomplete_sent == True:\n",
    "#     sent_raw = []\n",
    "#     sent_label = []\n",
    "#     cnt = 0\n",
    "#     for i, row in enumerate(raw_data):\n",
    "#         row_nlp = nlp(row)\n",
    "\n",
    "#     #     if len(raw_data[i].split()) >= 2: # 길이가 최소 2이상인 문장들마나 선별하자. (ex. .만 있는 문장도 있다)\n",
    "#     #         temp = []\n",
    "#     #         for token in row_nlp:\n",
    "#     #             temp.append(token.pos_) # 각 token의 pos 저장\n",
    "#     #         if 'VERB' in temp: # 문장에 최소 1개이상의 verb가 있어야 한다.\n",
    "#     #             if len([x for x in label_data[i].split() if x != 'O']) != 0: # 엔티티가 하나도 없으면 안된다.\n",
    "#     #                 sent_raw.append(raw_data[i])\n",
    "#     #                 sent_label.append(label_data[i])  \n",
    "\n",
    "#         if raw_data[i][-1] == '.' or raw_data[i][-1] == '\"': # 마지막에 쉼표가 있는 문장들만 선별하자.\n",
    "#             if len(raw_data[i].split()) >= 2: # 길이가 최소 2이상인 문장들마나 선별하자. (ex. .만 있는 문장도 있다)\n",
    "#                 temp = []\n",
    "#                 for token in row_nlp:\n",
    "#                     temp.append(token.pos_) # 각 token의 pos 저장\n",
    "#                 if 'VERB' in temp: # 문장에 최소 1개이상의 verb가 있어야 한다.\n",
    "#                     if len([x for x in label_data[i].split() if x != 'O']) != 0: # 엔티티가 하나도 없으면 안된다.\n",
    "#                         sent_raw.append(raw_data[i])\n",
    "#                         sent_label.append(label_data[i]) \n",
    "#     #                 else:\n",
    "#     #                     cnt += 1\n",
    "\n",
    "# sent_raw = raw_data[:]\n",
    "# sent_label = label_data[:]\n",
    "\n",
    "if filter_none_entity == True:\n",
    "    sent_raw, sent_label = filtering_none_entity(raw_data, label_data)\n",
    "else:\n",
    "    sent_raw = raw_data[:]\n",
    "    sent_label = label_data[:]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_with_word2vec(word, pos):\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)\n",
    "    num_upbound = 10\n",
    "    nlp_word = nlp.vocab[word]\n",
    "\n",
    "    ### similarity가 비슷한 단어들 추출\n",
    "    queries = [w for w in nlp_word.vocab if w.is_lower == nlp_word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: nlp_word.similarity(w), reverse=True)\n",
    "    #print([w.lower_ for w in by_similarity[:num_upbound]])\n",
    "    cand_word_list = [w.lower_ for w in by_similarity[:num_upbound]] # some candidate words\n",
    "    #print(cand_word_list)\n",
    "    \n",
    "    ### 후보군들 중에서 어떤 것을 교체어로 선정할 것인가?\n",
    "    # stemming과 lemmatization을 모두 사용하자.\n",
    "    source_stem = stemmer.stem((word))\n",
    "    source_lemma = lemmatiser.lemmatize((word), pos)\n",
    "    \n",
    "    for i, token in enumerate(cand_word_list):\n",
    "        #print(source_stem, stemmer.stem(str(token)), source_lemma, lemmatiser.lemmatize(str(token), pos))\n",
    "        if source_stem == stemmer.stem((token)):\n",
    "            cand_word_list[i] = 'to-be-deleted'\n",
    "        if source_lemma == lemmatiser.lemmatize((token), pos):\n",
    "            cand_word_list[i] = 'to-be-deleted'\n",
    "    cand_word_list = [x for x in cand_word_list if x != 'to-be-deleted'] # 한번에 삭제\n",
    "    \n",
    "    print(cand_word_list[0], nlp_word.similarity(nlp.vocab[cand_word_list[0]]))\n",
    "    \n",
    "    #print(cand_word_list)\n",
    "    # 다 삭제하고 empty면 그냥 교체하지 말자. (ex. are: ['are', 'were', 'werent', \"'re\", 'is'])\n",
    "    if len(cand_word_list)==0:\n",
    "        return word\n",
    "    else:\n",
    "        return cand_word_list[0] # 후보군 중에서 가장 맨 앞에 있는 단어 선택\n",
    "\n",
    "\n",
    "def replace_with_sense2vec(word, pos):\n",
    "    global empty_num_sense2vec\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)\n",
    "    num_upbound = 5\n",
    "    try:\n",
    "        freq, query_vector = model[word+'|'+pos]\n",
    "    except KeyError:\n",
    "        empty_num_sense2vec += 1\n",
    "        return 'sense2vec is empty!!!'\n",
    "    return model.most_similar(query_vector, n=num_upbound)\n",
    "\n",
    "\n",
    "def replace_with_wordnet(word, pos):\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)   \n",
    "        \n",
    "    dog_synsets = wn.synsets(word,pos) # wordnet은 이렇게 POS도 인자로 넣을 수 있다\n",
    "    for i, syn in enumerate(dog_synsets):\n",
    "        print('%d. %s' % (i, syn.name()))\n",
    "        print('alternative names (lemmas): \"%s\"' % '\", \"'.join(syn.lemma_names()))\n",
    "        \n",
    "        #print('definition: \"%s\"' % syn.definition())\n",
    "        #if syn.examples():\n",
    "        #    print('example usage: \"%s\"' % '\", \"'.join(syn.examples()))\n",
    "        \n",
    "def preprocessing_for_spacy(sent_raw):\n",
    "    ### 문장 string 전체 단위\n",
    "    sent_raw = sent_raw.replace(\"'ve\", 'have')\n",
    "    ### 문장 내에 있는 token 단위\n",
    "    # nlp output과 sent_raw의 길이가 같도록 하기 위해 전처리 실시\n",
    "    # ex. nlp()를 거치면, EU-wide와 같은 단어는 EU, -, wide로 3개로 분리된다. \n",
    "    splited_sent_raw = sent_raw.split()\n",
    "    for i, token in enumerate(splited_sent_raw):\n",
    "        splited_token = token.split('-')\n",
    "        if not splited_token == 1: # 일반 단어들이 아니라면, result. 'EU-wide', '--'\n",
    "            if not (splited_token[0] == '' or splited_token[-1] =='km'): # '-'으로만 이뤄진 단어가 아니라면, result.'EU-wide'\n",
    "                filtered_token = [x for x in splited_token if x != '-'] # '-'를 list에서 삭제 result.['EU', 'wide']\n",
    "                merged_token = ''.join(filtered_token)\n",
    "                splited_sent_raw[i] = merged_token\n",
    "    \n",
    "    for i, token in enumerate(splited_sent_raw):\n",
    "        if not len(token) == 1: # . 한개만 있는 token은 제외\n",
    "            splited_sent_raw[i] = splited_sent_raw[i].replace('.', '')\n",
    "            splited_sent_raw[i] = splited_sent_raw[i].replace('$', '')\n",
    "        if token == 'cannot':\n",
    "            splited_sent_raw[i] = \"can\"\n",
    "        if token == 'dont':\n",
    "            splited_sent_raw[i] = \"do\"\n",
    "        if token == \"'re\":\n",
    "            splited_sent_raw[i] = \"are\"\n",
    "        if token == \"**\":\n",
    "            splited_sent_raw[i] = \"*\"        \n",
    "        if token == \"...\" or token == \"..\" or token == \"....\":\n",
    "            splited_sent_raw[i] = \".\"         \n",
    "        if token == \"'m\":\n",
    "            splited_sent_raw[i] = \"am\"         \n",
    "        if token == \"'ll\":\n",
    "            splited_sent_raw[i] = \"will\" \n",
    "        if token == \"'d\":\n",
    "            splited_sent_raw[i] = \"would\"\n",
    "            \n",
    "        # very specific problem of this task    \n",
    "        if token == \"*Note\":\n",
    "            splited_sent_raw[i] = \"Note\"   \n",
    "        if token == \"*Name\":\n",
    "            splited_sent_raw[i] = \"Name\" \n",
    "        if token == 'km':\n",
    "            splited_sent_raw[i] = \"miles\"             \n",
    "        if token == '237km':\n",
    "            splited_sent_raw[i] = \"237-km\"             \n",
    "    return ' '.join(splited_sent_raw)\n",
    "    \n",
    "    \n",
    "def wordnet(word, pos):\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)   \n",
    "    synsets = wn.synsets(word, pos)\n",
    "    list_synsets = []\n",
    "    for i, syn in enumerate(synsets):\n",
    "        #print('%d. %s' % (i, syn.name()))\n",
    "        li = (syn.lemma_names())\n",
    "        list_synsets += (li)\n",
    "        for hyper_syn in syn.hypernyms():\n",
    "            list_synsets += (hyper_syn.lemma_names())\n",
    "        for hypo_syn in syn.hyponyms():\n",
    "            list_synsets += (hypo_syn.lemma_names())\n",
    "        for holo_syn in syn.part_holonyms():\n",
    "            list_synsets += (holo_syn.lemma_names())\n",
    "        for mero_syn in syn.part_meronyms():\n",
    "            list_synsets += (mero_syn.lemma_names())\n",
    "    \n",
    "    # 똑같은 stem 제거\n",
    "    source_stem = stemmer.stem(word)\n",
    "    source_lemma = lemmatiser.lemmatize(word, pos)\n",
    "    for i, token in enumerate(list_synsets):\n",
    "        #print(source_stem, stemmer.stem(str(token)), source_lemma, lemmatiser.lemmatize(str(token), pos))\n",
    "        if source_stem == stemmer.stem(token):\n",
    "            list_synsets[i] = 'to-be-deleted'\n",
    "        if source_lemma == lemmatiser.lemmatize(token, pos):\n",
    "            list_synsets[i] = 'to-be-deleted'\n",
    "        if len(token.split('_')) != 1: # 'bill_of_exchange'와 같이 명사구 또는 복합명사일 경우... 삭제...\n",
    "            list_synsets[i] = 'to-be-deleted'\n",
    "    list_synsets = [x for x in list_synsets if x != 'to-be-deleted'] # 한번에 삭제      \n",
    "    return list_synsets\n",
    "    \n",
    "    \n",
    "def word2vec(word, pos):\n",
    "    thr = 0.1\n",
    "    thr = thr / 2.0\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)\n",
    "    n_candidates = 30\n",
    "    nlp_word = nlp.vocab[word]\n",
    "    \n",
    "    ### similarity가 비슷한 단어들 추출\n",
    "    queries = [w for w in nlp_word.vocab if w.is_lower == nlp_word.is_lower and w.prob >= -11]\n",
    "    by_similarity = sorted(queries, key=lambda w: nlp_word.similarity(w), reverse=True)\n",
    "    #print(len([w.lower_ for w in by_similarity[:]]))\n",
    "    #print(([w.lower_ for w in by_similarity[:]]))\n",
    "    cand_word_list = [w.lower_ for w in by_similarity[:n_candidates]] # some candidate words\n",
    "    #print(cand_word_list)\n",
    "    \n",
    "    ### 후보군들 중에서 어떤 것을 교체어로 선정할 것인가?\n",
    "    # stemming과 lemmatization을 모두 사용하자.\n",
    "    source_stem = stemmer.stem(word)\n",
    "    source_lemma = lemmatiser.lemmatize(word, pos)\n",
    "    \n",
    "    for i, token in enumerate(cand_word_list):\n",
    "        #print(source_stem, stemmer.stem(str(token)), source_lemma, lemmatiser.lemmatize(str(token), pos))\n",
    "        if source_stem == stemmer.stem(token):\n",
    "            cand_word_list[i] = 'to-be-deleted'\n",
    "        if source_lemma == lemmatiser.lemmatize(token, pos):\n",
    "            cand_word_list[i] = 'to-be-deleted'\n",
    "    cand_word_list = [x for x in cand_word_list if x != 'to-be-deleted'] # 한번에 삭제\n",
    "\n",
    "    #print(cand_word_list)\n",
    "    #print('\\n')\n",
    "    if len(cand_word_list) == 0:\n",
    "        return False\n",
    "    if nlp_word.similarity(nlp.vocab[cand_word_list[0]]) > thr: # 70%로 비슷하면 \n",
    "        #return cand_word_list[0] # 최상위 1개 선택\n",
    "        return random.choice(cand_word_list) # 랜덤으로 선택\n",
    "    else:\n",
    "        list_wordnet = wordnet(word, pos)\n",
    "        cand_word_list_filtered_from_wornet = [v for v in cand_word_list if v in list_wordnet]\n",
    "        if len(cand_word_list_filtered_from_wornet)==0:\n",
    "            if len(list_wordnet)==0:\n",
    "                return False\n",
    "            else:\n",
    "                return list_wordnet[0]\n",
    "        else:\n",
    "            #print(cand_word_list_filtered_from_wornet)\n",
    "            #print('\\n')\n",
    "            return random.choice(cand_word_list_filtered_from_wornet)\n",
    "            #return cand_word_list_filtered_from_wornet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_algorithm(list_sent_raw, list_rep, pos):\n",
    "    bit = False\n",
    "    new_list_sent_raw = list_sent_raw[:]\n",
    "    for token, _, idx in list_rep:\n",
    "        \n",
    "        result = word2vec(token, pos)\n",
    "        if result != False: # 실패하지 않은 경우에만...\n",
    "            bit = True\n",
    "            new_list_sent_raw[idx] = result\n",
    "        \n",
    "    return new_list_sent_raw, bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def window_filtering(sent_label, window_size=3):\n",
    "    list_sent_label = sent_label.split()\n",
    "    new_list = [0] * len(list_sent_label)\n",
    "    \n",
    "    ### 순방향\n",
    "    for i, _ in enumerate(list_sent_label):\n",
    "        temp = list_sent_label[i:i+1+window_size]\n",
    "        #print(temp)\n",
    "        \n",
    "        tkn = False\n",
    "        for each in temp:\n",
    "            if each != 'O':\n",
    "                tkn = True\n",
    "        if tkn == True:\n",
    "            new_list[i] = 1\n",
    "             \n",
    "    ### 역방향\n",
    "    for i  in range(len(list_sent_label)-1, -1, -1):\n",
    "        idx = i-window_size\n",
    "        if idx < 0:\n",
    "            idx = 0\n",
    "        temp = list_sent_label[idx:i]\n",
    "        \n",
    "        tkn = False\n",
    "        for each in temp:\n",
    "            if each != 'O':\n",
    "                tkn = True\n",
    "        if tkn == True:\n",
    "            new_list[i] = 1      \n",
    "        \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generation_via_replace(sent_raw, sent_label, alpha):\n",
    "\n",
    "    #############\n",
    "    \"\"\" 전처리 \"\"\"\n",
    "    #############\n",
    "    # 동사, 명사는 context 정보의 핵심이다 모든 언어가 공통적으로 가지고 있는 특징.  \n",
    "    #print(sent_raw)\n",
    "    list_sent_raw = sent_raw.split()\n",
    "    sent_raw = preprocessing_for_spacy(sent_raw)\n",
    "    sent_nlp = nlp(sent_raw)\n",
    "    noun_sorted, verb_sorted, adj_sorted, adv_sorted = [], [], [], []\n",
    "    \n",
    " \n",
    "    ### assert 에러 났을 때, 원인 찾기 위해 print 실시\n",
    "    if not len(sent_nlp) == len(sent_label.split()):\n",
    "        print(len(sent_raw.split()), len(sent_nlp), len(sent_label.split()))\n",
    "        print(sent_raw)\n",
    "        print(sent_nlp)\n",
    "        for i, token in enumerate(sent_nlp):\n",
    "            print(token, sent_raw.split()[i], sent_label.split()[i])\n",
    "    assert(len(sent_nlp) == len(sent_label.split()))\n",
    "    \n",
    "    ### filtering with window size based on entities\n",
    "    filtered_list = window_filtering(sent_label)\n",
    "    \n",
    "    ### 조건에 맞는 token들을 불러온다 \n",
    "    for i, token in enumerate(sent_nlp):\n",
    "\n",
    "        ### 제외되는 token들..\n",
    "        if not sent_label.split()[i] == 'O': # 엔티티가 아닌 token은 제외\n",
    "            continue\n",
    "        is_uppercase_letter = True in map(lambda l: l.isupper(), str(token))\n",
    "        if i != 0 and is_uppercase_letter == True: # 첫 번째 글자가 아니고 대문자가 하나라도 있는 token은 제외 (ex. Thursday와 같은 시간고유명사. 크게 의미적이지 않음.)\n",
    "            continue\n",
    "        if lemmatiser.lemmatize(str(token), 'v') == 'be': # be동사 lemma를 가지는 token은 제외\n",
    "            continue\n",
    "        if lemmatiser.lemmatize(str(token), 'v') == 'have': # have 동사 lemma를 가지는 token은 제외, (have 동사의 의미가 광범위하게 쓰이기도 하고, have가 분사형으로 사용되기도 하기 때문이다)\n",
    "            continue\n",
    "           \n",
    "        if filtered_list[i] == 0: # filtering with window size based on entities\n",
    "            continue\n",
    "            \n",
    "            \n",
    "    \n",
    "        ### 동사, 명사, 형용사, 부사 추출\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            pair = (sent_nlp[i], len(list(token.subtree)), i) \n",
    "            noun_sorted.append(pair) # 명사\n",
    "               \n",
    "        elif token.pos_ == 'VERB':\n",
    "            pair = (sent_nlp[i], len(list(token.subtree)), i)\n",
    "            verb_sorted.append(pair) # 동사       \n",
    "        \n",
    "        elif token.pos_ == 'ADJ':\n",
    "            pair = (sent_nlp[i], len(list(token.subtree)), i)\n",
    "            adj_sorted.append(pair) # 형용사         \n",
    "\n",
    "        elif token.pos_ == 'ADV':\n",
    "            pair = (sent_nlp[i], len(list(token.subtree)), i)\n",
    "            adv_sorted.append(pair) # 부사  \n",
    "            \n",
    "    \n",
    "    ### dependency 크기로 정렬\n",
    "    noun_sorted.sort(key=lambda x: x[1], reverse=True)\n",
    "    verb_sorted.sort(key=lambda x: x[1], reverse=True)\n",
    "    adj_sorted.sort(key=lambda x: x[1], reverse=True)\n",
    "    adv_sorted.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     print((noun_sorted))\n",
    "#     print((verb_sorted))\n",
    "    if len(noun_sorted)==0 and len(verb_sorted)==0 and len(adj_sorted)==0 and len(adv_sorted)==0:\n",
    "        return False # 교체 후보 token이 아예없다면, 새로운 문장을 만들 수 없다.\n",
    "\n",
    "    ### thr 정하기\n",
    "    noun_thr = int(len(noun_sorted) * alpha)\n",
    "    verb_thr = int(len(verb_sorted) * alpha)\n",
    "    adj_thr = int(len(adj_sorted) * alpha)\n",
    "    adv_thr = int(len(adv_sorted) * alpha)\n",
    "    \n",
    "    if noun_thr == 0:\n",
    "        noun_thr = 1 # 최소값은 1로 유지\n",
    "    if verb_thr == 0:\n",
    "        verb_thr = 1 # 최소값은 1로 유지\n",
    "    if adj_thr == 0:\n",
    "        adj_thr = 1 # 최소값은 1로 유지\n",
    "    if adv_thr == 0:\n",
    "        adv_thr = 1 # 최소값은 1로 유지        \n",
    "    \n",
    "    ### token list to be replaced\n",
    "    noun_rep = noun_sorted[:noun_thr] \n",
    "    verb_rep = verb_sorted[:verb_thr] \n",
    "    adj_rep = adj_sorted[:adj_thr]\n",
    "    adv_rep = adv_sorted[:adv_thr]\n",
    "    \n",
    "    \n",
    "    #print(noun_rep)\n",
    "    #print(verb_rep)\n",
    "    #print(list_sent_raw)\n",
    "    \n",
    "    if len(noun_sorted) != 0: # 각자 0이 될 수도 있으니...\n",
    "        list_sent_raw, bit_n = replace_algorithm(list_sent_raw, noun_rep, 'n')\n",
    "        #print(list_sent_raw)\n",
    "        #replaced_noun = (noun_sorted[0][0], noun_sorted[0][2]) # (token, index) pair \n",
    "        # 명사\n",
    "        #print('<<< ', replaced_noun[0], '  >>>')\n",
    "        #target_noun = replace_with_word2vec(replaced_noun[0], 'n')\n",
    "        #print(replace_with_sense2vec(replaced_noun[0], 'NOUN'))\n",
    "        #print(replace_with_wordnet(replaced_noun[0], 'n'))\n",
    "        #print('replaced_noun: ', replaced_noun, ', target_noun: ', target_noun)\n",
    "        #list_sent_raw[replaced_noun[1]] = target_noun # 교체\n",
    "    else:\n",
    "        bit_n = False\n",
    "        \n",
    "    \n",
    "    if len(verb_sorted) != 0: # 각자 0이 될 수도 있으니...\n",
    "        list_sent_raw, bit_v = replace_algorithm(list_sent_raw, verb_rep, 'v')\n",
    "        #print(list_sent_raw)\n",
    "        #replaced_verb = (verb_sorted[0][0], verb_sorted[0][2])\n",
    "        # 동사\n",
    "        #print('<<< ', replaced_verb[0], '  >>>')\n",
    "        #target_verb = replace_with_word2vec(replaced_verb[0], 'v')\n",
    "        #print(replace_with_sense2vec(replaced_verb[0], 'VERB'))\n",
    "        #print(replace_with_wordnet(replaced_verb[0], 'v'))   \n",
    "        #print('replaced_verb: ', replaced_verb, ', target_verb: ', target_verb)\n",
    "        #list_sent_raw[replaced_verb[1]] = target_verb # 교체\n",
    "    else:\n",
    "        bit_v = False\n",
    "    \n",
    "    \n",
    "    if len(adj_sorted) != 0: # 각자 0이 될 수도 있으니...\n",
    "        list_sent_raw, bit_a = replace_algorithm(list_sent_raw, adj_rep, 'a')\n",
    "    else:\n",
    "        bit_a = False    \n",
    "    \n",
    "    \n",
    "    if len(adv_sorted) != 0: # 각자 0이 될 수도 있으니...\n",
    "        list_sent_raw, bit_r = replace_algorithm(list_sent_raw, adv_rep, 'r')\n",
    "    else:\n",
    "        bit_r = False \n",
    "        \n",
    "    \n",
    "    if bit_n==False and bit_v==False and bit_a==False and bit_a==False:\n",
    "        return False\n",
    "    else:    \n",
    "        #print(' '.join(list_sent_raw))\n",
    "        return ' '.join(list_sent_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt_replace_word = 0\n",
    "new_sent_raw_list = []\n",
    "new_sent_label_list = [] \n",
    "idx = 0\n",
    "\n",
    "for i, row in enumerate(sent_raw):\n",
    "\n",
    "    #if random.choice([True, False, False, False]) == False:\n",
    "    #    continue\n",
    "    #print(sent_raw[i])\n",
    "    #print(sent_label[i])\n",
    "    \n",
    "    #alpha_list = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    #alpha = random.choice(alpha_list)\n",
    "    alpha = 0.9\n",
    "    result = generation_via_replace(sent_raw[i], sent_label[i], alpha)\n",
    "    \n",
    "    if result == False: # 교체어가 없었는 경우\n",
    "        continue\n",
    "    else:\n",
    "        cnt_replace_word += 1\n",
    "        new_sent_raw_list.append(result)\n",
    "        new_sent_label_list.append(sent_label[i]) # 라벨은 동일하다.\n",
    "\n",
    "#         print(', \\n')\n",
    "#         idx += 1\n",
    "#     #     print('-------------------------------------------------------------------------')\n",
    "#         if idx==10:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnt_replace_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store new sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_write = 'data_logic_warehouse/type_2/replace_1_'+str(len(new_sent_raw_list))+'.txt'\n",
    "\n",
    "with open(path_write, 'w', encoding='UTF-8') as txt:\n",
    "    \n",
    "    for i, _ in enumerate(new_sent_raw_list):\n",
    "        splited_sent = new_sent_raw_list[i].split()\n",
    "        splited_label = new_sent_label_list[i].split()\n",
    "        for j, token in enumerate(splited_sent):\n",
    "            txt.write(splited_sent[j]+' '+'NNP'+' '+'B-NP'+' '+splited_label[j])\n",
    "            txt.write('\\n')\n",
    "        txt.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float('0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "worde2vec, globe 사이에 얼만큼 차이가 존재하나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity(self, other):\n",
    "    self = nlp.vocab[self]\n",
    "    other = nlp.vocab[other]\n",
    "    if self.vector_norm == 0 or other.vector_norm == 0:\n",
    "        return 0.0\n",
    "    return numpy.dot(self.vector, other.vector) / (self.vector_norm * other.vector_norm)\n",
    "\n",
    "print(similarity('accumulate', 'gradually'))\n",
    "print(similarity('accumulate', 'slowly'))\n",
    "print(similarity('accumulate', 'steadily'))\n",
    "print(similarity('accumulate', 'rapidly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(similarity('accuse', 'practically'))\n",
    "print(similarity('accuse', 'virtually'))\n",
    "print(similarity('accuse', 'falsely'))\n",
    "print(similarity('accuse', 'unjustly'))\n",
    "print(similarity('accuse', 'wrongly'))\n",
    "print(similarity('accuse', 'angrily'))\n",
    "print(similarity('accuse', 'openly'))\n",
    "print(similarity('accuse', 'publicly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(similarity('wonder', 'wide-eyed'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity(self, other):\n",
    "    self = nlp.vocab[self]\n",
    "    other = nlp.vocab[other]\n",
    "    if self.vector_norm == 0 or other.vector_norm == 0:\n",
    "        return 0.0\n",
    "    return numpy.dot(self.vector, other.vector) / (self.vector_norm * other.vector_norm)\n",
    "\n",
    "print(similarity('accumulate', 'gradually'))\n",
    "print(similarity('accumulate', 'slowly'))\n",
    "print(similarity('accumulate', 'steadily'))\n",
    "print(similarity('accumulate', 'rapidly'))\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def spacy_max_similarity(w1, w2):\n",
    "    word_1 = nlp.vocab[w1]\n",
    "    word_2 = nlp.vocab[w2]\n",
    "    return word_1.similarity(word_2)\n",
    "\n",
    "print(spacy_max_similarity('call', 'call'))\n",
    "\n",
    "import numpy as np\n",
    "apples = nlp.vocab[u\"the\"]\n",
    "print(apples.prob)\n",
    "print(apples.vector_norm)\n",
    "# prints 1.4142135381698608, or sqrt(2)\n",
    "print(np.sqrt(np.dot(apples.vector, apples.vector)))\n",
    "# prints 1.0\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def replace_with_word2vec2(word):\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)\n",
    "    num_upbound = 200\n",
    "    word = nlp.vocab[word]\n",
    "    print(word.prob)\n",
    "    # Most similar ones\n",
    "    #queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    queries = [w for w in word.vocab if w.prob >= -15]\n",
    "    print(len(queries))\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    #print(by_similarity)\n",
    "    #print([w.lower_ for w in by_similarity[:num_upbound]])\n",
    "    cand_word_list = [w.lower_ for w in by_similarity[:num_upbound]] # some candidate words\n",
    "#     sim_list = [w for w in by_similarity[:num_upbound]]\n",
    "    #print(sim_list)\n",
    "    print(cand_word_list)\n",
    "    \n",
    "replace_with_word2vec2('Sheep')\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "class AntonymReplacer(object):\n",
    "    def replace(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "        for syn in wn.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def replace_negations(self, sent):\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i+1 < l:\n",
    "                ant = self.replace(sent[i+1])\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "        return words\n",
    "    \n",
    "replacer = AntonymReplacer()\n",
    "print(replacer.replace('beautiful'))\n",
    "print(replacer.replace('uglify'))\n",
    "\n",
    "#sent = [\"let's\", 'not', 'uglify', 'our', 'code']\n",
    "#replacer.replace_negations(sent)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "synsets = wn.synsets('measures', 'n')\n",
    "for i, syn in enumerate(synsets):\n",
    "\n",
    "    print('%d. %s' % (i, syn.name()))\n",
    "    print('alternative names (lemmas): \"%s\"' % '\", \"'.join(syn.lemma_names()))\n",
    "    print('hypErnyms: ', syn.hypernyms())\n",
    "    print('hypOrnyms: ', syn.hyponyms())\n",
    "    print('part_holonyms: ', syn.part_holonyms())\n",
    "    print('part_meronyms: ', syn.part_meronyms())\n",
    "#     print('test: ', syn.())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replace_with_word2vec2('measures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replace_with_word2vec2('beautifully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = 'About 350 adventurers from nine countries set out on Saturday to climb , raft , bike and run in a 323-mile ( 517-km ) endurance race through the Canadian wilderness .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = nlp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for token in nlp:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for token in nlp:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = [('c', 2), ('a',1), ('c', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(temp, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"He is been here before.\")\n",
    "for token in doc:\n",
    "    print(token, len(list(token.subtree)), token.pos_, lemmatiser.lemmatize((str(token)), 'v'))\n",
    "    #print(np.text, np.root.text, np.root.dep_, np.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "s_cnt = 0\n",
    "for sent in raw_data:\n",
    "    cnt += 1\n",
    "    if list(sent)[-1] == '.':\n",
    "        s_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 어떤 것을 교체할 것인가?\n",
    "# 전략1: 모두 교체\n",
    "# 전략2: 전치사(closed-class words) 빼고 다 교체\n",
    "# 전략3: 명사, 동사만 교체\n",
    "# 전략4: dependency가 높은 명사, 동사만 교체\n",
    "# 엔티티의 head\n",
    "# 전략5: root token만 교체\n",
    "\n",
    "### 어떻게 교체할 것인가?\n",
    "# word2vec을 사용해서\n",
    "# wordnet을 사용해서\n",
    "# sense2vec 사용하면 임베딩에서 태그정보도 같이 넣을 수 있음.\n",
    "\n",
    "# NER사전을 통해 다른 엔티티로 교체 (NER에 한해서만) - https://explosion.ai/blog/sense2vec-with-spacy\n",
    "## -> 그러면 엔티티에 대한 패턴을 좀 더 확장할 수 있지 않을까\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
