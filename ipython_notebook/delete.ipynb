{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete할 때, parser tree result를 이용해서 문장을 축약해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy\n",
    "import sense2vec\n",
    "import random\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from data_handling_for_heuristic import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "nlp = spacy.load('en')\n",
    "#model = sense2vec.load()\n",
    "\n",
    "import os\n",
    "os.environ['STANFORD_PARSER'] = 'C:\\\\stanford-parser-full-2016-10-31\\\\stanford-parser.jar'\n",
    "os.environ['STANFORD_MODELS'] = 'C:\\\\stanford-parser-full-2016-10-31\\\\stanford-parser-3.7.0-models.jar'\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "cons_parser = StanfordParser(model_path = 'edu\\\\stanford\\\\nlp\\\\models\\\\lexparser\\\\englishPCFG.ser.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #read_path = 'data/conll2003/eng.train'\n",
    "# read_path = 'train.txt'\n",
    "# with open(read_path, \"r\") as ins:\n",
    "#     raw_data = []\n",
    "#     label_data = []\n",
    "    \n",
    "#     temp_sent = ''\n",
    "#     temp_label = ''\n",
    "#     tkn = False\n",
    "    \n",
    "#     for line in ins:\n",
    "#         #array.append(line)\n",
    "\n",
    "#         if len(line) == 1:\n",
    "#             raw_data.append(temp_sent)\n",
    "#             label_data.append(temp_label)\n",
    "#             temp_sent = ''\n",
    "#             temp_label = ''\n",
    "#             tkn = False\n",
    "#         else:\n",
    "#             if tkn == True:\n",
    "#                 temp_sent += ' '\n",
    "#                 temp_label += ' '\n",
    "                \n",
    "#             temp_sent += line.split()[0] # 단어\n",
    "#             temp_label += line.split()[-1] # NER 라벨\n",
    "#             tkn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filtering_none_entity]: before = 14987 and after = 11132\n"
     ]
    }
   ],
   "source": [
    "sent_raw = []\n",
    "sent_label = []\n",
    "cnt = 0\n",
    "for i, row in enumerate(raw_data):\n",
    "    row_nlp = nlp(row)\n",
    "\n",
    "#     if len(raw_data[i].split()) >= 2: # 길이가 최소 2이상인 문장들마나 선별하자. (ex. .만 있는 문장도 있다)\n",
    "#         temp = []\n",
    "#         for token in row_nlp:\n",
    "#             temp.append(token.pos_) # 각 token의 pos 저장\n",
    "#         if 'VERB' in temp: # 문장에 최소 1개이상의 verb가 있어야 한다.\n",
    "#             if len([x for x in label_data[i].split() if x != 'O']) != 0: # 엔티티가 하나도 없으면 안된다.\n",
    "#                 sent_raw.append(raw_data[i])\n",
    "#                 sent_label.append(label_data[i]) \n",
    " \n",
    "    \n",
    "    \n",
    "    if raw_data[i][-1] == '.' or raw_data[i][-1] == '\"': # 마지막에 쉼표가 있는 문장들만 선별하자.\n",
    "        if len(raw_data[i].split()) >= 2: # 길이가 최소 2이상인 문장들마나 선별하자. (ex. .만 있는 문장도 있다)\n",
    "            temp = []\n",
    "            for token in row_nlp:\n",
    "                temp.append(token.pos_) # 각 token의 pos 저장\n",
    "            if 'VERB' in temp: # 문장에 최소 1개이상의 verb가 있어야 한다.\n",
    "                if len([x for x in label_data[i].split() if x != 'O']) != 0: # 엔티티가 하나도 없으면 안된다.\n",
    "                    sent_raw.append(raw_data[i])\n",
    "                    sent_label.append(label_data[i]) \n",
    "#                 else:\n",
    "#                     cnt += 1\n",
    "\n",
    "read_file_path = 'data/conll2003/train.txt'\n",
    "raw_data, label_data = load_conll2003(read_file_path)\n",
    "\n",
    "filter_none_entity = True\n",
    "if filter_none_entity == True:\n",
    "    sent_raw, sent_label = filtering_none_entity(raw_data, label_data)\n",
    "else:\n",
    "    sent_raw = raw_data[:]\n",
    "    sent_label = label_data[:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent_raw: 문장 단위 + 정상적인 문장 텍스트 <br>\n",
    "sent_label: 문장 단위 + 정상적인 문장 텍스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from data_handling_for_heuristic import *\n",
    "# #read_path = 'data/conll2003/eng.train'\n",
    "# read_path = 'data_logic_warehouse/type_2/merged_replace.txt'\n",
    "# #read_path = 'train.txt'\n",
    "# sent_raw, sent_label = load_conll2003(read_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11132"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Preprocessing\n",
    "\n",
    "def del_ent_in_list(delete_list, X):\n",
    "# token 단위로 확인하면서 삭제한다\n",
    "# call to ( boycott ) 이면 (과 )는 삭제된다. token단위이므로... (token단위 기준: space)\n",
    "# X: ['EU rejects German call to boycott British lamb .', ... ]\n",
    "# Y: ['I-ORG O I-MISC O O O I-MISC O O', ... ]\n",
    "# delete_list = [')','(', ';', ...]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "        if any(str(X[i]) in t for t in delete_list):\n",
    "            #del Y[i]\n",
    "            del X[i]\n",
    "        #else:\n",
    "        i+=1               \n",
    "    return X #, Y\n",
    "\n",
    "def del_ent_in_string(delete_list, X):\n",
    "# char 단위로 확인하면서 삭제한다\n",
    "# 예를 들어, boycott이 아니라 boy(cott이면 해당 token은 삭제된다. 이처럼 char레벨까지 체크한다.\n",
    "# X: ['EU rejects German call to boycott British lamb .', ... ]\n",
    "# Y: ['I-ORG O I-MISC O O O I-MISC O O', ... ]\n",
    "# delete_list = [')','(', ';', ...]    \n",
    "    i=0\n",
    "    while i < len(X):\n",
    "        #print(str(sp_test[i]))\n",
    "        list_X = list(X[i])\n",
    "        if len(list_X)==1:\n",
    "            break\n",
    "        for char in list_X:\n",
    "            if any(char in t for t in delete_list):\n",
    "                #print(X)\n",
    "                #print(Y)\n",
    "                #del Y[i]\n",
    "                del X[i]\n",
    "                break\n",
    "        #else:\n",
    "        i+=1\n",
    "    return X#, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global index_SBAR\n",
    "global list_SBAR \n",
    "\n",
    "# index_SBAR = -1\n",
    "# list_SBAR = [0] * len(sent_raw[3].split())\n",
    "\n",
    "def traverseTree(tree, temp_sentence):\n",
    "\n",
    "    global index_SBAR\n",
    "    global list_SBAR    \n",
    "    \n",
    "#     if tree.label() == 'ROOT': # 처음에 initialization\n",
    "#         index_SBAR = -1\n",
    "#         list_SBAR = [0] * len(sent_list)\n",
    "\n",
    "    #print(\"tree:\", tree)\n",
    "    #print(\"======> tree.pos()\", tree.pos())\n",
    "    #print(\"======> tree.height()\", tree.height())\n",
    "    #print(\"======> tree.label()\", tree.label())\n",
    "    #print(\"======> tree.leaves()\", tree.leaves())\n",
    "    \n",
    "    #print(\"======> current_depth\", current_depth)\n",
    "    #print('======> current depth list', list_current_depth)\n",
    "\n",
    "    #print('*************************************************************************************************')    \n",
    "    #print('\\n')\n",
    "\n",
    "    if type(tree) == nltk.tree.Tree:\n",
    "        \n",
    "        # Initialication\n",
    "        current_height = tree.height()\n",
    "\n",
    "        \n",
    "        ### 로직 한계점\n",
    "        ### 만약 SBAR안에 SBAR이 있는 경우 구분하지 못한다. 또는 SBAR이 또다른 SBAR과 바로 옆에 붙어있는 경우 구분하지 못한다.\n",
    "        ### 그러면 나중에 제일 첫 글자만 참고해서 어떤 부사절인지 보는데, 만약 2개 이상 SBAR이 포함되어있거나 붙어있는 경우\n",
    "        ### index가 제일 앞에 있는 SBAR만 참고하게 된다. (예를 들어, because SBAR이 뒤에 있는 경우 무시하게 된다)\n",
    "        \n",
    "        # SBAR\n",
    "        if tree.label() == 'SBAR':\n",
    "            #print('SBAR start!!!')\n",
    "            \n",
    "            if len(tree.leaves()) <= 1: # SBAR token개수가 1개 이하이면 return. \n",
    "                return None\n",
    "            \n",
    "            for i, token in enumerate(temp_sentence):\n",
    "                #print('temp_sentence[i]:', temp_sentence[i], ', tree.leaves()[0]:',tree.leaves()[0]) \n",
    "                #print('temp_sentence[i+1]:', temp_sentence[i+1], ', tree.leaves()[1]:', tree.leaves()[1])\n",
    "                \n",
    "                \n",
    "                #if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:\n",
    "                if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+1] == tree.leaves()[1]:\n",
    "                    #print('break!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                    index_SBAR = i\n",
    "                    break\n",
    "                \n",
    "            if not index_SBAR == -1:\n",
    "                for j in range(index_SBAR, index_SBAR+len(tree.leaves())):\n",
    "                    list_SBAR[j] = 1   \n",
    "                    \n",
    "            return None\n",
    "\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            traverseTree(subtree, temp_sentence) # recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def re_numbering(list_SBAR):\n",
    "# ex. [1,1,1,0,0,1,1,0,1,1] => [1,1,1,0,0,2,2,0,3,3]\n",
    "# 후처리하기 쉽게하기 위해 numbering을 다시한다.\n",
    "    \n",
    "    cur_idx = 1\n",
    "    new_list_SBAR = [0] * len(list_SBAR)\n",
    "    \n",
    "    for i, _ in enumerate(list_SBAR):\n",
    "        if list_SBAR[i]!=0:\n",
    "            new_list_SBAR[i] = cur_idx\n",
    "        if not i==len(list_SBAR)-1:\n",
    "            if list_SBAR[i]!=0 and list_SBAR[i+1]==0:\n",
    "                cur_idx += 1\n",
    "    return new_list_SBAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global cnt_MainClause_delete_SBARwithoutENT\n",
    "global cnt_AdverbialClause_reuse_SBAR\n",
    "global cnt_MainClause_delete_SBARwithENT\n",
    "\n",
    "cnt_MainClause_delete_SBARwithoutENT = 0 # SBAR에 엔티티가 없는경우 그냥 삭제.\n",
    "cnt_AdverbialClause_reuse_SBAR = 0 # SBAR이 완전한 문장이고,  그 안에 엔티티가 있는경우 재사용.\n",
    "cnt_MainClause_delete_SBARwithENT = 0\n",
    "\n",
    "def gen_with_SBAR(list_SBAR, token_raw_list, token_label_list):\n",
    "    \n",
    "    global cnt_MainClause_delete_SBARwithoutENT\n",
    "    global cnt_AdverbialClause_reuse_SBAR\n",
    "    global cnt_MainClause_delete_SBARwithENT\n",
    "    \n",
    "    \n",
    "    # 주의: list_SBAR, sent_raw_list, sent_label_list의 길이는 모두 같다.\n",
    "    \n",
    "    sent_clauses = ['because', 'before', 'until', 'after', 'while', 'if', 'since', 'when', 'as', 'where',\n",
    "                   'Because', 'Before', 'Until', 'After', 'While', 'If', 'Since', 'When', 'As'] \n",
    "    noun_clauses = ['which', 'who', 'to']\n",
    "    # to는 to부정사를 지칭하는 거임. 좀 더 정확하게 하려면 다음에 동사인 것을 확인해야 하지만, SBAR인점과 첫 글자가 to이라는 것은 to부정사 확률이 높음.\n",
    "    # 명사절에서 that절은 빼자. that이 워낙 경우의 수가 다양해서.. 일단은 배제하자.\n",
    "    \n",
    "    new_raw_sent_list = [] # 문장 여러개 \n",
    "    new_label_sent_list = []\n",
    "    \n",
    "    #print(token_raw_list)\n",
    "    #print(list_SBAR)\n",
    "    #print('------------------------------------------')    \n",
    "\n",
    "    for i, _ in enumerate(list_SBAR):\n",
    "        temp_raw_str = ''\n",
    "        temp_label_str = ''\n",
    "        \n",
    "\n",
    "        \n",
    "        ##########################\n",
    "        ###### @@ 로직 똑같음 ######\n",
    "        ##########################\n",
    "        if i==0 and list_SBAR[i] != 0: # 앞 boundary일 경우의 수 1\n",
    "        ##########################    \n",
    "        ##########################    \n",
    "            if str(token_raw_list[i]) in sent_clauses: # 삭제, 분리\n",
    "                \n",
    "                # 엔티티가 해당 clause에 있는지 확인\n",
    "                is_ent_in_clause = False\n",
    "                for k, _ in enumerate(list_SBAR):\n",
    "                    if list_SBAR[k]==list_SBAR[i]: # 해당 번호\n",
    "                        if token_label_list[k]!='O': # 엔티티가 있다는 뜻\n",
    "                            is_ent_in_clause = True\n",
    "                            \n",
    "                if is_ent_in_clause == False: # 엔티티가 없으면.. 해당 clause 삭제\n",
    "                    for m, n in enumerate(list_SBAR):\n",
    "                        if n != list_SBAR[i]: # clause를 삭제하자. 즉, clause에 속하지 않는 것들만 추리자.\n",
    "                            temp_raw_str += str(token_raw_list[m])\n",
    "                            temp_raw_str += ' '\n",
    "                            temp_label_str += str(token_label_list[m])\n",
    "                            temp_label_str += ' '\n",
    "                            \n",
    "                    ### clause가 삭제된 문장 저장\n",
    "                    #print(temp_raw_str)\n",
    "                    cnt_MainClause_delete_SBARwithoutENT += 1\n",
    "                    new_raw_sent_list.append(temp_raw_str)\n",
    "                    new_label_sent_list.append(temp_label_str)\n",
    "                    \n",
    "                if is_ent_in_clause == True: # 엔티티가 있으면..\n",
    "                    fir_tkn = False\n",
    "                    for m, n in enumerate(list_SBAR):\n",
    "                        if n == list_SBAR[i]: # clause에 속하는 것들만 추리자\n",
    "                            if fir_tkn == False: # 첫 번째 글자는 pass하자. (ex. because, if, while 등등)\n",
    "                                fir_tkn = True\n",
    "                                continue\n",
    "                            temp_raw_str += str(token_raw_list[m])\n",
    "                            temp_raw_str += ' '\n",
    "                            temp_label_str += str(token_label_list[m])\n",
    "                            temp_label_str += ' '\n",
    "                            \n",
    "                    ### clause가 삭제된 문장 저장\n",
    "                    #print(temp_raw_str)\n",
    "                    cnt_AdverbialClause_reuse_SBAR += 1\n",
    "                    new_raw_sent_list.append(temp_raw_str)\n",
    "                    new_label_sent_list.append(temp_label_str)\n",
    "                    \n",
    "                    # 엔티티가 clause외에도 있는지 확인\n",
    "                    is_ent_out_clause = False\n",
    "                    for k, _ in enumerate(list_SBAR):\n",
    "                        if list_SBAR[k]!=list_SBAR[i]: # 해당 번호\n",
    "                            if token_label_list[k]!='O': # 엔티티가 있다는 뜻\n",
    "                                is_ent_out_clause = True                \n",
    "                    \n",
    "                    # 엔티티가 clause외에 있으면 그것도 데이터확장에 사용.\n",
    "                    temp_raw_str = ''\n",
    "                    temp_label_str = ''\n",
    "                    if is_ent_out_clause == True:\n",
    "                        for m, n in enumerate(list_SBAR):\n",
    "                            if n != list_SBAR[i]: # clause에 속하지 않는 것들만 추리자.\n",
    "                                temp_raw_str += str(token_raw_list[m])\n",
    "                                temp_raw_str += ' '\n",
    "                                temp_label_str += str(token_label_list[m])\n",
    "                                temp_label_str += ' '\n",
    "\n",
    "                        ### clause가 삭제된 문장 저장\n",
    "                        #print(temp_raw_str)\n",
    "                        cnt_MainClause_delete_SBARwithENT += 1\n",
    "                        new_raw_sent_list.append(temp_raw_str)\n",
    "                        new_label_sent_list.append(temp_label_str)                \n",
    "                \n",
    "                \n",
    "\n",
    "            elif str(token_raw_list[i]) in noun_clauses: # 삭제만함.\n",
    "\n",
    "                # 엔티티가 해당 clause에 있는지 확인\n",
    "                is_ent_in_clause = False\n",
    "                for k, _ in enumerate(list_SBAR):\n",
    "                    if list_SBAR[k]==list_SBAR[i]: # 해당 번호\n",
    "                        if token_label_list[k]!='O': # 엔티티가 있다는 뜻\n",
    "                            is_ent_in_clause = True\n",
    "                            \n",
    "                if is_ent_in_clause == False: # 엔티티가 없으면.. 해당 clause 삭제\n",
    "                    for m, n in enumerate(list_SBAR):\n",
    "                        if n != list_SBAR[i]: # clause를 삭제하자. 즉, clause에 속하지 않는 것들만 추리자.\n",
    "                            temp_raw_str += str(token_raw_list[m])\n",
    "                            temp_raw_str += ' '\n",
    "                            temp_label_str += str(token_label_list[m])\n",
    "                            temp_label_str += ' '\n",
    "                            \n",
    "                    ### clause가 삭제된 문장 저장\n",
    "                    #print(temp_raw_str)\n",
    "                    cnt_MainClause_delete_SBARwithoutENT += 1\n",
    "                    new_raw_sent_list.append(temp_raw_str)\n",
    "                    new_label_sent_list.append(temp_label_str)                \n",
    "                \n",
    "\n",
    "        \n",
    "                \n",
    "        ##########################                \n",
    "        ###### @@ 로직 똑같음 ######\n",
    "        ##########################            \n",
    "        if i != 0:\n",
    "            if list_SBAR[i-1]==0 and list_SBAR[i] != 0: # 앞 boundary일 경우의 수 2\n",
    "        ##########################\n",
    "        ##########################\n",
    "        \n",
    "                if str(token_raw_list[i]) in sent_clauses: # 삭제, 분리\n",
    "\n",
    "\n",
    "                    # 엔티티가 해당 clause에 있는지 확인\n",
    "                    is_ent_in_clause = False\n",
    "                    for k, _ in enumerate(list_SBAR):\n",
    "                        if list_SBAR[k]==list_SBAR[i]: # 해당 번호\n",
    "                            if token_label_list[k]!='O': # 엔티티가 있다는 뜻\n",
    "                                is_ent_in_clause = True\n",
    "\n",
    "                    if is_ent_in_clause == False: # 엔티티가 없으면.. 해당 clause 삭제\n",
    "                        for m, n in enumerate(list_SBAR):\n",
    "                            if n != list_SBAR[i]: # clause를 삭제하자. 즉, clause에 속하지 않는 것들만 추리자.\n",
    "                                temp_raw_str += str(token_raw_list[m])\n",
    "                                temp_raw_str += ' '\n",
    "                                temp_label_str += str(token_label_list[m])\n",
    "                                temp_label_str += ' '\n",
    "\n",
    "                        ### clause가 삭제된 문장 저장\n",
    "                        #print(temp_raw_str)\n",
    "                        cnt_MainClause_delete_SBARwithoutENT += 1\n",
    "                        new_raw_sent_list.append(temp_raw_str)\n",
    "                        new_label_sent_list.append(temp_label_str)\n",
    "\n",
    "                    if is_ent_in_clause == True: # 엔티티가 있으면..\n",
    "                        fir_tkn = False\n",
    "                        for m, n in enumerate(list_SBAR):\n",
    "                            if n == list_SBAR[i]: # clause에 속하는 것들만 추리자\n",
    "                                if fir_tkn == False: # 첫 번째 글자는 pass하자. (ex. because, if, while 등등)\n",
    "                                    fir_tkn = True\n",
    "                                    continue\n",
    "                                temp_raw_str += str(token_raw_list[m])\n",
    "                                temp_raw_str += ' '\n",
    "                                temp_label_str += str(token_label_list[m])\n",
    "                                temp_label_str += ' '\n",
    "\n",
    "                        ### clause가 삭제된 문장 저장\n",
    "                        #print(temp_raw_str)\n",
    "                        cnt_AdverbialClause_reuse_SBAR += 1\n",
    "                        new_raw_sent_list.append(temp_raw_str)\n",
    "                        new_label_sent_list.append(temp_label_str)\n",
    "\n",
    "                        # 엔티티가 clause외에도 있는지 확인\n",
    "                        is_ent_out_clause = False\n",
    "                        for k, _ in enumerate(list_SBAR):\n",
    "                            if list_SBAR[k]!=list_SBAR[i]: # 해당 번호\n",
    "                                if token_label_list[k]!='O': # 엔티티가 있다는 뜻\n",
    "                                    is_ent_out_clause = True                \n",
    "\n",
    "                        # 엔티티가 clause외에 있으면 그것도 데이터확장에 사용.\n",
    "                        temp_raw_str = ''\n",
    "                        temp_label_str = ''\n",
    "                        if is_ent_out_clause == True:\n",
    "                            for m, n in enumerate(list_SBAR):\n",
    "                                if n != list_SBAR[i]: # clause에 속하지 않는 것들만 추리자.\n",
    "                                    temp_raw_str += str(token_raw_list[m])\n",
    "                                    temp_raw_str += ' '\n",
    "                                    temp_label_str += str(token_label_list[m])\n",
    "                                    temp_label_str += ' '\n",
    "\n",
    "                            ### clause가 삭제된 문장 저장\n",
    "                            #print(temp_raw_str)\n",
    "                            cnt_MainClause_delete_SBARwithENT += 1\n",
    "                            new_raw_sent_list.append(temp_raw_str)\n",
    "                            new_label_sent_list.append(temp_label_str)                \n",
    "\n",
    "\n",
    "\n",
    "                elif str(token_raw_list[i]) in noun_clauses: # 삭제만함.\n",
    "\n",
    "                    # 엔티티가 해당 clause에 있는지 확인\n",
    "                    is_ent_in_clause = False\n",
    "                    for k, _ in enumerate(list_SBAR):\n",
    "                        if list_SBAR[k]==list_SBAR[i]: # 해당 번호\n",
    "                            if token_label_list[k]!='O': # 엔티티가 있다는 뜻\n",
    "                                is_ent_in_clause = True\n",
    "\n",
    "                    if is_ent_in_clause == False: # 엔티티가 없으면.. 해당 clause 삭제\n",
    "                        for m, n in enumerate(list_SBAR):\n",
    "                            if n != list_SBAR[i]: # clause를 삭제하자. 즉, clause에 속하지 않는 것들만 추리자.\n",
    "                                temp_raw_str += str(token_raw_list[m])\n",
    "                                temp_raw_str += ' '\n",
    "                                temp_label_str += str(token_label_list[m])\n",
    "                                temp_label_str += ' '\n",
    "\n",
    "                        # clause가 삭제된 문장 저장\n",
    "                        #print(temp_raw_str)\n",
    "                        cnt_MainClause_delete_SBARwithoutENT += 1\n",
    "                        new_raw_sent_list.append(temp_raw_str)\n",
    "                        new_label_sent_list.append(temp_label_str)                \n",
    "                \n",
    "    \n",
    "    return new_raw_sent_list, new_label_sent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 괄호가 있어도 상관은 없는데..\n",
    "# tree.leaves()에서는 괄호는 출력되지 않는다.\n",
    "# 하지만1, string안에 괄호가 있는 경우는 parser 오류가 난다. 예를 들어 ap(ple인 단어가 있으면 오류가 난다. \n",
    "# 하지만2, 괄호가 있지만, 쌍으로 존재하지 않을경우 parser 오류가 난다. 예를 들어, ), ( 이렇게 한 개씩만 있는 경우...\n",
    "# 즉, 한 문장에서 괄호가 pair로 존재하지 않는 경우는 삭제하자.!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [\n",
    "'While', \n",
    "'57',\n",
    "'percent',\n",
    "'of',\n",
    "'the',\n",
    "'population',\n",
    "'supported',\n",
    "'negotiations',\n",
    "'with',\n",
    "'ETA',\n",
    "'(',\n",
    "'Basque',\n",
    "'Homeland',\n",
    "'and',\n",
    "'Freedom',\n",
    "')',\n",
    "',',\n",
    "'30',\n",
    "'percent',\n",
    "'opposed',\n",
    "'it',\n",
    "',',\n",
    "'the',\n",
    "'survey',\n",
    "'by',\n",
    "'the',\n",
    "'state-controlled',\n",
    "'Centre',\n",
    "'for',\n",
    "'Sociological',\n",
    "'Studies',\n",
    "'(',\n",
    "'CIS',\n",
    "')',\n",
    "'found',\n",
    "'.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [\n",
    "'British', \n",
    "'Jewish', \n",
    "'groups', \n",
    "'have', \n",
    "'also', \n",
    "'voiced', \n",
    "'protest', \n",
    "'because', \n",
    "'they', \n",
    "'said', \n",
    "'Palestinian', \n",
    "'Islamist',\n",
    "'Hamas',\n",
    "'as', \n",
    "'well',\n",
    "'as',\n",
    "'the',\n",
    "'banned',\n",
    "'Algerian',\n",
    "'Islamic',\n",
    "'Salvation',\n",
    "'Front',\n",
    "'(',\n",
    "'FIS',\n",
    "')',\n",
    "'are',\n",
    "'among',\n",
    "'those',\n",
    "'radical',\n",
    "'Islamists',\n",
    "'attending',\n",
    "'the',\n",
    "'conference',\n",
    "'.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0a183ed2f204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[1;31m#pos_sent_raw_list = nltk.pos_tag(token_raw_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[1;31m### parsing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mparse_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcons_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_sent_raw_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[1;31m###################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Office\\Anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36mtagged_parse\u001b[0;34m(self, sentence, verbose)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_parse_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtagged_parse_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Office\\Anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36mtagged_parse_sents\u001b[0;34m(self, sentences, verbose)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[1;31m# We don't need to escape slashes as \"splitting is done on the last instance of the character in the token\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         return self._parse_trees_output(self._execute(\n\u001b[0;32m--> 187\u001b[0;31m             cmd, '\\n'.join(' '.join(tag_separator.join(tagged) for tagged in sentence) for sentence in sentences), verbose))\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Office\\Anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, cmd, input_, verbose)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 stdout, stderr = java(cmd, classpath=self._classpath,\n\u001b[0;32m--> 216\u001b[0;31m                                       stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mstdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'\\xc2\\xa0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mb' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Office\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[1;31m# Check the return code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Office\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Office\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[1;31m# calls communicate again.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remaining_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Office\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Office\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_sent_list = []\n",
    "new_label_list = [] \n",
    "\n",
    "for i, _ in enumerate(sent_raw):\n",
    "    \n",
    "#     print('***********************************')\n",
    "#     print(sent_raw[i])\n",
    "    \n",
    "    token_raw_list = sent_raw[i].split()\n",
    "    #token_raw_list = test\n",
    "    token_label_list = sent_label[i].split()\n",
    "    \n",
    "    #print(test)\n",
    "    \n",
    "    \n",
    "    #print(sent_raw[i])\n",
    "    #print(sent_label[i])\n",
    "    \n",
    "    if '(' in token_raw_list:\n",
    "        continue\n",
    "    if ')' in token_raw_list:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ### 괄호 처리\n",
    "#     parenthesis_list = []\n",
    "#     for i, _ in enumerate(token_raw_list):\n",
    "#         if token_raw_list[i]=='(' or token_raw_list[i]==')':\n",
    "#             parenthesis_list.append((token_raw_list[i], i)) # append (pair)\n",
    "\n",
    "#     ### preprocessing\n",
    "#     pre_raw_list = del_ent_in_list(['(',')'], token_raw_list)\n",
    "#     pre2_raw_list = del_ent_in_string(['(',')'], token_raw_list) # 한 문장을 위해 실시.\n",
    "    \n",
    "    #print(pos_sent_raw_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### pos tagging\n",
    "    pos_sent_raw_list = nltk.pos_tag(token_raw_list)\n",
    "    #pos_sent_raw_list = nltk.pos_tag(token_raw_list)\n",
    "    ### parsing\n",
    "    parse_result = cons_parser.tagged_parse(pos_sent_raw_list)\n",
    "\n",
    "    ###################################\n",
    "    \"\"\" SBAR, 부사절 삭제 및 분리 \"\"\"\n",
    "    ###################################\n",
    "    # initialization\n",
    "    index_SBAR = -1\n",
    "    list_SBAR = [0] * len(token_raw_list)    \n",
    "    \n",
    "    for tree in parse_result:\n",
    "        parse_tree = tree\n",
    "        traverseTree(parse_tree, token_raw_list)# tree traverse하면서 SBAR tree label을 찾음.\n",
    "    \n",
    "    #print(list_SBAR)\n",
    "    \n",
    "    ###################################\n",
    "    \"\"\" 괄호 복구 작업 \"\"\"\n",
    "    ###################################\n",
    "#     for pair in parenthesis_list: # 순서대로 리스트업 했기 때문에 순서대로 insert하기만 하면 됨. \n",
    "#         parenthesis = pair[0]\n",
    "#         index = pair[1]\n",
    "#         # token_raw_list에 괄호 복구\n",
    "#         token_raw_list.insert(index, parenthesis)\n",
    "#         # SBAR list에 괄호 복구\n",
    "#         if parenthesis == '(':\n",
    "#             if list_SBAR[index+1] == 1:\n",
    "#                 list_SBAR.insert(index, 1)\n",
    "#             else:\n",
    "#                 list_SBAR.insert(index, 0)\n",
    "#         elif parenthesis == ')':\n",
    "#             if list_SBAR[index-1] == 1:\n",
    "#                 list_SBAR.insert(index, 1)\n",
    "#             else:\n",
    "#                 list_SBAR.insert(index, 0)\n",
    "            \n",
    "    ##############################################\n",
    "    \"\"\" Boundary 검사: 엔티티가 쪼개져 있으면 수정 \"\"\"    \n",
    "    ##############################################\n",
    "    # parsing 결과를 보정\n",
    "    tkn = True\n",
    "    while(tkn):\n",
    "        \n",
    "        btn = False\n",
    "        for i, num in enumerate(list_SBAR):\n",
    "            if i!=len(list_SBAR)-1:\n",
    "                if list_SBAR[i] == 1 and list_SBAR[i+1] == 0:\n",
    "                    btn = True\n",
    "        if btn == False and list_SBAR[-1] == 1:\n",
    "            tkn = False\n",
    "\n",
    "        if not 1 in list_SBAR:\n",
    "            tkn = False\n",
    "            \n",
    "        for i, num in enumerate(list_SBAR):\n",
    "            if i!=0: # 첫 번째 index는 제외\n",
    "                if list_SBAR[i-1] == 1 and list_SBAR[i] == 0: # find SBAR boundary!\n",
    "                    if token_label_list[i-1] != 'O': # 엔티티라면\n",
    "                        if token_label_list[i].split('-')[0] == 'I': # (좀 더 구체적으로..) INSIDE 계열의 엔티티라면..\n",
    "                            list_SBAR[i] = 1\n",
    "                        else: # BEGIN 계열이면 그냥 pass\n",
    "                            tkn = False # break\n",
    "                    else: # 엔티티가 아니라면\n",
    "                        tkn = False # break\n",
    "\n",
    "    #print('hi')\n",
    "    \n",
    "                        \n",
    "    ### for printf \n",
    "#     print(list_SBAR)\n",
    "#     if 1 in list_SBAR:\n",
    "#         temp = ''\n",
    "#         for k, tkn in enumerate(token_raw_list):\n",
    "#             if list_SBAR[k] == 1:\n",
    "#                 temp = temp+token_raw_list[k]+' '\n",
    "#         print(temp)\n",
    "              \n",
    "            \n",
    "            \n",
    "    # generation with SBAR\n",
    "    if 1 in list_SBAR: # 현재 문장에서 SBAR이 있으면...\n",
    "        # renumbering\n",
    "        list_SBAR = re_numbering(list_SBAR)\n",
    "        new_sent_raw_list, new_sent_label_list = gen_with_SBAR(list_SBAR, token_raw_list, token_label_list)\n",
    "        #print(new_sent_raw_list)\n",
    "        #print('\\n')\n",
    "        \n",
    "        if len(new_sent_raw_list) != 0: # 새로운 문장이 생성되었는가?\n",
    "            # 주의: append가 아니라 +로해야됨. new_sent_list가 list이기 때문에\n",
    "            new_sent_list += new_sent_raw_list\n",
    "            new_label_list += new_sent_label_list\n",
    "            \n",
    "    # 엔티티 포함되어 있으면 냅둔다.\n",
    "\n",
    "#     for i, _ in enumerate(list_SBAR):\n",
    "\n",
    "#         if not i==0:\n",
    "#             if list_SBAR[i-1]==0 and list_SBAR[i]==1:\n",
    "\n",
    "#         else: # i==0일 때...\n",
    "\n",
    "        #print('------------------------------------------------')            \n",
    "        #print(sent_raw[i])\n",
    "        #print('------------------------------------------------')\n",
    "        #print(temp)\n",
    "        #print(list_SBAR)\n",
    "        #print('------------------------------------------------')\n",
    "    #print('\\n')\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2054 2054\n"
     ]
    }
   ],
   "source": [
    "print(len(new_sent_list), len(new_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777\n",
      "717\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "print(cnt_MainClause_delete_SBARwithoutENT)\n",
    "print(cnt_AdverbialClause_reuse_SBAR)\n",
    "print(cnt_MainClause_delete_SBARwithENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store new sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2054"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_write = 'data_logic_warehouse/type_2/merged_replace_delete_'+str(len(new_sent_list))+'.txt'\n",
    "\n",
    "with open(path_write, 'w', encoding='UTF-8') as txt:\n",
    "    \n",
    "    for i, _ in enumerate(new_sent_list):\n",
    "        splited_sent = new_sent_list[i].split()\n",
    "        splited_label = new_label_list[i].split()\n",
    "        for j, token in enumerate(splited_sent):\n",
    "            txt.write(splited_sent[j]+' '+'NNP'+' '+'B-NP'+' '+splited_label[j])\n",
    "            txt.write('\\n')\n",
    "        txt.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ### 테스트용\n",
    "# test = 'Stanley owns a .367 career batting average with the bases loaded ( 33-for-90 ) because I have coffee .'\n",
    "# test = 'The tournament also lost its second seed on the third day of play when second-seeded Goran Ivanisevic of Croatia was beaten 6-7(3-7 6-4 6-4 by unseeded Mikael Tillstrom of Sweden .'\n",
    "# token_test_list = test.split()\n",
    "# pos_token_test_list = nltk.pos_tag(token_test_list)\n",
    "# parse_result = cons_parser.tagged_parse(pos_token_test_list)\n",
    "\n",
    "# global index_SBAR\n",
    "# global list_SBAR \n",
    "\n",
    "# index_SBAR = -1\n",
    "# list_SBAR = [0] * len(test.split())\n",
    "\n",
    "# def traverseTree1(tree, temp_sentence):\n",
    "\n",
    "#     global index_SBAR\n",
    "#     global list_SBAR \n",
    "# #     if tree.label() == 'ROOT': # 처음에 initialization\n",
    "# #         index_SBAR = -1\n",
    "# #         list_SBAR = [0] * len(sent_list)\n",
    "#     #print(\"======> temp_sentence\", temp_sentence)\n",
    "#     #print(\"tree:\", tree)\n",
    "# #     print(\"======> tree.pos()\", tree.pos())\n",
    "# #     print(\"======> tree.height()\", tree.height())\n",
    "# #     print(\"======> tree.label()\", tree.label())\n",
    "#     #print(\"======> tree.leaves()\", tree.leaves())\n",
    "    \n",
    "#     #print(\"======> current_depth\", current_depth)\n",
    "#     #print('======> current depth list', list_current_depth)\n",
    "\n",
    "# #     print('*************************************************************************************************')    \n",
    "# #     print('\\n')\n",
    "\n",
    "#     if type(tree) == nltk.tree.Tree:\n",
    "        \n",
    "#         # Initialication\n",
    "#         current_height = tree.height()\n",
    "\n",
    "#         # SBAR\n",
    "#         if tree.label() == 'SBAR':\n",
    "#             print(\"tree:\", tree)\n",
    "#             print(tree.leaves())\n",
    "#             for i, token in enumerate(temp_sentence):\n",
    "#                 print(i, temp_sentence[i], tree.leaves()[0], '<->', temp_sentence[i+len(tree.leaves())-1], tree.leaves()[-1])\n",
    "#                 #if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+len(tree.leaves())-1] == tree.leaves()[-1]:  \n",
    "#                 if temp_sentence[i] == tree.leaves()[0] and temp_sentence[i+1] == tree.leaves()[1]:  \n",
    "                    \n",
    "#                     index_SBAR = i\n",
    "#                     print('**********************************************************')\n",
    "#                     break  \n",
    "                \n",
    "#             if not index_SBAR == -1:\n",
    "#                 for j in range(index_SBAR, index_SBAR+len(tree.leaves())):\n",
    "#                     list_SBAR[j] = 1 \n",
    "        \n",
    "  \n",
    "\n",
    "#     for subtree in tree:\n",
    "#         if type(subtree) == nltk.tree.Tree:\n",
    "#             traverseTree1(subtree, temp_sentence) # recursive\n",
    "            \n",
    "# for tree in parse_result:\n",
    "#     parse_tree = tree\n",
    "#     traverseTree1(parse_tree, token_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if using_parsing == 1:\n",
    "#     #########################\n",
    "#     \"\"\" For training data \"\"\"\n",
    "#     #########################\n",
    "#     cnt = 0\n",
    "#     temp_depth_list = []\n",
    "#     temp_n_sibling_list = []\n",
    "#     for i, sentence in enumerate(pre_X_train):\n",
    "#         temp = nltk.pos_tag(sentence)\n",
    "#         par_result = parser.tagged_parse(temp)\n",
    "#         for tree in par_result:\n",
    "#             parse_tree = tree\n",
    "#         #print(parse_tree)\n",
    "\n",
    "#         # For PP, NP, VP,...\n",
    "#         temp_sentence = sentence\n",
    "#         initial_tree_parameter(sentence)\n",
    "#         traverseTree(parse_tree) # For getting tree information\n",
    "\n",
    "#         # For depth, n_sibling...\n",
    "#         depth_list, n_siblings_list = tree_depth_and_n_sibling(parse_tree)\n",
    "#         assert(len(sentence) == len(depth_list))\n",
    "#     #     print(sentence)\n",
    "#     #     print(parse_tree.leaves())\n",
    "#         normalized_li_depth = normalizing_depth_list(depth_list)\n",
    "#         normalized_li_n_siblings = normalizing_n_siblings_list(n_siblings_list)\n",
    "\n",
    "#         for j, token in enumerate(sentence):\n",
    "#             ### Tree depth and sibling\n",
    "#             FE2_X_train[i][j] += FE_depth(j, sentence, normalized_li_depth, window_size_depth) \n",
    "#             FE2_X_train[i][j] += FE_n_siblings(j, sentence, normalized_li_n_siblings, window_size_n_siblings)     \n",
    "\n",
    "#             ### Phrases-based Features\n",
    "# #             FE2_X_train[i][j] += [list_NPSBAR[j]] # defalut 오직 SBAR만 뽑는다.\n",
    "# #             FE2_X_train[i][j] += [list_VPSBAR[j]]\n",
    "# #             FE2_X_train[i][j] += [list_SSBAR[j]]\n",
    "# #             FE2_X_train[i][j] += [list_SQSBAR[j]]\n",
    "\n",
    "# #             FE2_X_train[i][j] += [list_PPPP[j]] # default 오직 PP만 뽑는다.\n",
    "# #             FE2_X_train[i][j] += [list_VPPP[j]]\n",
    "# #             FE2_X_train[i][j] += [list_NPPP[j]] # sub_refinement_of_object를 추출하는데 도움..\n",
    "\n",
    "# #             FE2_X_train[i][j] += [list_NPNP[j]]\n",
    "# #             FE2_X_train[i][j] += [list_VPNP[j]] \n",
    "# #             FE2_X_train[i][j] += [list_SNP[j]] \n",
    "\n",
    "# #             FE2_X_train[i][j] += [list_VPVP[j]] \n",
    "# #             FE2_X_train[i][j] += [list_SVP[j]]      \n",
    "\n",
    "        \n",
    "#     #     cnt += 1\n",
    "#     #     print('===================================================================')\n",
    "#     #     if cnt == 1:\n",
    "#     #         break\n",
    "\n",
    "#     ########################\n",
    "#     \"\"\" For testing data \"\"\"\n",
    "#     ########################\n",
    "#     cnt = 0\n",
    "#     for i, sentence in enumerate(pre_X_test):\n",
    "#         temp = nltk.pos_tag(sentence)\n",
    "#         par_result = parser.tagged_parse(temp)\n",
    "#         for tree in par_result:\n",
    "#             parse_tree = tree\n",
    "#         #print(parse_tree)\n",
    "\n",
    "#         # For PP, NP, VP, ...\n",
    "#         temp_sentence = sentence\n",
    "#         initial_tree_parameter(sentence)\n",
    "#         traverseTree(parse_tree) # For getting tree information\n",
    "\n",
    "#         # For dpeth, n_sibling\n",
    "#         depth_list, n_siblings_list = tree_depth_and_n_sibling(parse_tree)\n",
    "#         assert(len(sentence) == len(depth_list))\n",
    "#     #     print(sentence)\n",
    "#     #     print(parse_tree.leaves())\n",
    "#         normalized_li_depth = normalizing_depth_list(depth_list)\n",
    "#         normalized_li_n_siblings = normalizing_n_siblings_list(n_siblings_list)\n",
    "\n",
    "#         for j, token in enumerate(sentence):\n",
    "\n",
    "#             ### Tree-depth, Sibling \n",
    "#             FE2_X_test[i][j] += FE_depth(j, sentence, normalized_li_depth, window_size_depth) \n",
    "#             FE2_X_test[i][j] += FE_n_siblings(j, sentence, normalized_li_n_siblings, window_size_n_siblings) \n",
    "\n",
    "#             ## Phrases-based Festures\n",
    "# #             FE2_X_test[i][j] += [list_NPSBAR[j]]\n",
    "# #             FE2_X_test[i][j] += [list_VPSBAR[j]]\n",
    "# #             FE2_X_test[i][j] += [list_SSBAR[j]]\n",
    "# #             FE2_X_test[i][j] += [list_SQSBAR[j]]\n",
    "\n",
    "# #             FE2_X_test[i][j] += [list_PPPP[j]]\n",
    "# #             FE2_X_test[i][j] += [list_VPPP[j]]\n",
    "# #             FE2_X_test[i][j] += [list_NPPP[j]] \n",
    "\n",
    "# #             FE2_X_test[i][j] += [list_NPNP[j]]\n",
    "# #             FE2_X_test[i][j] += [list_VPNP[j]] \n",
    "# #             FE2_X_test[i][j] += [list_SNP[j]] \n",
    "\n",
    "# #             FE2_X_test[i][j] += [list_VPVP[j]] \n",
    "# #             FE2_X_test[i][j] += [list_SVP[j]]\n",
    "\n",
    "\n",
    "\n",
    "#     #     print(convert_dummy2(depth_list[j], 0))\n",
    "#     #     print(convert_dummy2(n_siblings_list[j], 1))\n",
    "#     #     print([list_SBAR[j]])\n",
    "#     #     print([list_VPPP[j]])\n",
    "#     #     print([list_NPPP[j]])\n",
    "\n",
    "#     #     cnt += 1\n",
    "#     #     print('===================================================================')\n",
    "#     #     if cnt == 1:\n",
    "#     #         break\n",
    "       \n",
    "# #     print('===> current feature length', len(FE2_X_train[0][0])-len_pos_feature-len_chunk_feature)\n",
    "# #     len_parsing_feature = len(FE2_X_train[0][0])-len_pos_feature-len_chunk_feature\n",
    "# #     assert(len(FE2_X_train[0][0]) == len(FE2_X_test[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
