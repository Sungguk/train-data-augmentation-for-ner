{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert 목표: 문맥에 크게 영향을 주지 않는 선에서 단어들을 추가함.<br>\n",
    "즉, 다른 말로 하면 문맥을 유지한다는 말임.\n",
    "어떻게 보면 redundant한 단어들이 될 수 있지만, 이것 또한 LSTM에게 generalization 잘 할 수 있도록 해주는 동기가 되지 않을까? <br>\n",
    "\n",
    "바꾸는 문제 (교체)는 중의성 문제를 잘 해결해야 하지만,\n",
    "추가하는 문제는 중의성 문제가 크리티컬 하지 않다. (구문적 중의성만 해결할 수 있어도 괜찮지 않을까?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oxford Collocations\n",
    "\n",
    "추가할 수 있는 품사 : 부사, 형용사\n",
    "* 동사 기준으로 collocation 부사를 추가\n",
    "* 형용사 기준으로 collocation 부사를 추가\n",
    "* 명사 기준으로 collocation 형용사를 추가\n",
    "\n",
    "collocation 부사/형용사 list 중에 하나를 선택하는 일 역시 중의성 문제를 푸는 일이다. 어떤 기준을 가지겠는가? 우선 아무거나 선택해도 큰 문제는 없을 것이다. 의미가 많이 다르면, Oxford Collocation 사전에서는 번호를 매겨 구분지어 놓았다. 만약 번호로 구분지어 놓으면 해당 단어는 패스하자. 의미가 크게 다르지 않으면 단어면 '|'으로만 구분지어놓았는데 얘네들은 구분짓지 말고 리스트로 모아 랜덤으로 하나 선택해도 괜찮을 듯 하다. 그래도 선택할 수 있는 한 가지 척도가 있다면, word embedding의 similarity를 활용할 수도 있겠다. 완전히 의미적 중의성은 해결해주지 못하나 빈도수가 높은 애들은 선택할 수 있다는 데에 의의를 둘 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나중에 sense2vec과 google n-gram도 고려해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sense2vec의 결과가 phrase로 나오는게 많은데, 활용해보자.\n",
    "즉, 형용사를 insert했다고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내가 그냥 collocation verb-adverb 사전을 만들지 <br>\n",
    "아니면, nltk collocation (bigram, trigram)을 사용할지는 좀 더 고민해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.collocations import *\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# text = \"this is a foo bar bar black sheep  foo bar bar black sheep foo bar bar black  sheep shep bar bar black sentence\"\n",
    "\n",
    "# trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "# finder = TrigramCollocationFinder.from_words(word_tokenize(text))\n",
    "\n",
    "# for i in finder.score_ngrams(trigram_measures.pmi):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy\n",
    "#import sense2vec\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from data_handling_for_heuristic import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "nlp = spacy.load('en')\n",
    "#model = sense2vec.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #read_path = 'data/conll2003/eng.train'\n",
    "# read_path = 'train.txt'\n",
    "# with open(read_path, \"r\") as ins:\n",
    "#     raw_data = []\n",
    "#     label_data = []\n",
    "#     temp_sent = ''\n",
    "#     temp_label = ''\n",
    "#     tkn = False\n",
    "#     for line in ins:\n",
    "#         #array.append(line)\n",
    "#         if len(line) == 1:\n",
    "#             raw_data.append(temp_sent)\n",
    "#             label_data.append(temp_label)\n",
    "#             temp_sent = ''\n",
    "#             temp_label = ''\n",
    "#             tkn = False\n",
    "#         else:\n",
    "#             if tkn == True:\n",
    "#                 temp_sent += ' '\n",
    "#                 temp_label += ' '\n",
    "#             temp_sent += line.split()[0] # 단어\n",
    "#             temp_label += line.split()[-1] # NER 라벨\n",
    "#             tkn = True\n",
    "            \n",
    "# sent_raw = []\n",
    "# sent_label = []\n",
    "# cnt = 0\n",
    "# for i, row in enumerate(raw_data):\n",
    "#     row_nlp = nlp(row)\n",
    "\n",
    "# #     if len(raw_data[i].split()) >= 2: # 길이가 최소 2이상인 문장들마나 선별하자. (ex. .만 있는 문장도 있다)\n",
    "# #         temp = []\n",
    "# #         for token in row_nlp:\n",
    "# #             temp.append(token.pos_) # 각 token의 pos 저장\n",
    "# #         if 'VERB' in temp: # 문장에 최소 1개이상의 verb가 있어야 한다.\n",
    "# #             if len([x for x in label_data[i].split() if x != 'O']) != 0: # 엔티티가 하나도 없으면 안된다.\n",
    "# #                 sent_raw.append(raw_data[i])\n",
    "# #                 sent_label.append(label_data[i]) \n",
    " \n",
    "    \n",
    "    \n",
    "#     if raw_data[i][-1] == '.' or raw_data[i][-1] == '\"': # 마지막에 쉼표가 있는 문장들만 선별하자.\n",
    "#         if len(raw_data[i].split()) >= 2: # 길이가 최소 2이상인 문장들마나 선별하자. (ex. .만 있는 문장도 있다)\n",
    "#             temp = []\n",
    "#             for token in row_nlp:\n",
    "#                 temp.append(token.pos_) # 각 token의 pos 저장\n",
    "#             if 'VERB' in temp: # 문장에 최소 1개이상의 verb가 있어야 한다.\n",
    "#                 if len([x for x in label_data[i].split() if x != 'O']) != 0: # 엔티티가 하나도 없으면 안된다.\n",
    "#                     sent_raw.append(raw_data[i])\n",
    "#                     sent_label.append(label_data[i]) \n",
    "# #                 else:\n",
    "# #                     cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filtering_none_entity]: before = 14987 and after = 11132\n"
     ]
    }
   ],
   "source": [
    "read_file_path = '../data/conll2003/en/train.txt'\n",
    "raw_data, label_data = load_conll2003(read_file_path)\n",
    "\n",
    "filter_none_entity = True\n",
    "if filter_none_entity == True:\n",
    "    sent_raw, sent_label = filtering_none_entity(raw_data, label_data)\n",
    "else:\n",
    "    sent_raw = raw_data[:]\n",
    "    sent_label = label_data[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Oxford Collocation Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(name):\n",
    "    filehandler = open(name, \"rb\")\n",
    "    return pickle.load(filehandler)\n",
    "friend_list = load('resources/for_insert/friend_list')\n",
    "voca = load('../resources/for_insert/voca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_candidates = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(self, other):\n",
    "    self = nlp.vocab[self]\n",
    "    other = nlp.vocab[other]\n",
    "    if self.vector_norm == 0 or other.vector_norm == 0:\n",
    "        return 0.0\n",
    "    return numpy.dot(self.vector, other.vector) / (self.vector_norm * other.vector_norm)\n",
    "\n",
    "import numpy as np\n",
    "def max_friend(friend_list, token):\n",
    "    sim_score = [0] * len(friend_list)\n",
    "    for i, each in enumerate(friend_list):\n",
    "        sim_score[i] = similarity(token, friend_list[i])\n",
    "    \n",
    "    sim_score.sort(key=lambda x: x, reverse=True)\n",
    "    sim_score = sim_score[:n_candidates]\n",
    "    sim_score = list(filter(lambda x: x!= 0.0, sim_score))\n",
    "    if len(sim_score)==0:\n",
    "        return 'none'\n",
    "    \n",
    "    #print(sim_score)\n",
    "    #print('\\n')\n",
    "    #print(np.array(sim_score).max())\n",
    "\n",
    "    choiced_score = random.choice(sim_score)\n",
    "    #print(choiced_score)\n",
    "    \n",
    "    #print(np.array(sim_score).max(), '--', choiced_score)\n",
    "    \n",
    "    max1_index = np.where(sim_score == np.array(sim_score).max())[0][0]\n",
    "    max_index = np.where(sim_score == choiced_score)[0][0]\n",
    "    \n",
    "    #print(max1_index, max_index)\n",
    "    #print('\\n')\n",
    "\n",
    "    return friend_list[max_index]\n",
    "\n",
    "def insert_module(token, pos):\n",
    "    #print('------>',token, pos)\n",
    "    try:voca_index = voca.index((token, pos))\n",
    "    except(ValueError):\n",
    "        #print('*********>', 'none')\n",
    "        return 'none'\n",
    "    \n",
    "    if len(friend_list[voca_index])==0:\n",
    "        #print('*********>', 'none')\n",
    "        return 'none'\n",
    "    else:\n",
    "        new_token = max_friend(friend_list[voca_index], token)\n",
    "        #print('*********>',new_token)\n",
    "        return new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def window_filtering(sent_label, window_size=3):\n",
    "    list_sent_label = sent_label.split()\n",
    "    new_list = [0] * len(list_sent_label)\n",
    "    ### 순방향\n",
    "    for i, _ in enumerate(list_sent_label):\n",
    "        temp = list_sent_label[i:i+1+window_size]\n",
    "        #print(temp)\n",
    "        tkn = False\n",
    "        for each in temp:\n",
    "            if each != 'O':\n",
    "                tkn = True\n",
    "        if tkn == True:\n",
    "            new_list[i] = 1\n",
    "    ### 역방향\n",
    "    for i  in range(len(list_sent_label)-1, -1, -1):\n",
    "        idx = i-window_size\n",
    "        if idx < 0:\n",
    "            idx = 0\n",
    "        temp = list_sent_label[idx:i]\n",
    "        tkn = False\n",
    "        for each in temp:\n",
    "            if each != 'O':\n",
    "                tkn = True\n",
    "        if tkn == True:\n",
    "            new_list[i] = 1      \n",
    "    return new_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR]: len(list_sent_nlp) != len(list_sent_label)\n",
      "[ERROR]: len(list_sent_nlp) != len(list_sent_label)\n",
      "[ERROR]: len(list_sent_nlp) != len(list_sent_label)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-37260341f360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mnew_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minsert_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'adj'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[1;31m################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-13cbf15587d9>\u001b[0m in \u001b[0;36minsert_module\u001b[0;34m(token, pos)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minsert_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[1;31m#print('------>',token, pos)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mvoca_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvoca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[1;31m#print('*********>', 'none')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "insert_sent_cnt = 0\n",
    "new_sent_list = []\n",
    "new_label_list = [] \n",
    "\n",
    "\n",
    "\n",
    "for i, _ in enumerate(sent_raw):\n",
    "    \n",
    "    #list_sent_raw = sent_raw[i].split()\n",
    "    str_sent_raw = preprocessing_for_spacy(sent_raw[i])\n",
    "    list_sent_nlp = nlp(str_sent_raw)\n",
    "    \n",
    "    list_sent_raw = str_sent_raw.split()\n",
    "    list_sent_label = sent_label[i].split()\n",
    "    \n",
    "    #print(str_sent_raw)\n",
    "    \n",
    "    ###################################\n",
    "    \"\"\" 조건에 맞는 token들을 불러온다 \"\"\"\n",
    "    ###################################\n",
    "    pre_token_pos = 'none'\n",
    "    insert_on = False\n",
    "    add_idx = 0\n",
    "    \n",
    "    ### filtering with window size based on entities\n",
    "    filtered_list = window_filtering(sent_label[i])\n",
    "        \n",
    "    if len(list_sent_nlp) != len(list_sent_label):\n",
    "        print('[ERROR]: len(list_sent_nlp) != len(list_sent_label)')\n",
    "        continue\n",
    "    \n",
    "    for i, token in enumerate(list_sent_nlp):\n",
    "        \n",
    "        new_token = 'none' # 계속 'none'으로 남아있다면 추가하지 않을 것임.\n",
    "    \n",
    "        # 엔티티는 건드리지 말자.\n",
    "        \n",
    "        if list_sent_label[i+add_idx] != 'O':\n",
    "            continue\n",
    "        if filtered_list[i+add_idx] == 0: # filtering with window size based on entities\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #print('>>>', token, token.pos_, pre_token_pos)\n",
    "        #################\n",
    "        ### INSERT_MODULE  \n",
    "        str_token = str(token).lower()\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN': # 오로지 하나의 명사일경우...\n",
    "            if i==0:\n",
    "                str_token = lemmatiser.lemmatize(str_token, 'n')\n",
    "                new_token = insert_module(str_token, 'noun')\n",
    "            else:\n",
    "                if pre_token_pos != 'ADJ':\n",
    "                    if pre_token_pos != 'NOUN':\n",
    "                        if pre_token_pos != 'PROPN':\n",
    "                            str_token = lemmatiser.lemmatize(str_token, 'n')\n",
    "                            new_token = insert_module(str_token, 'noun')\n",
    "\n",
    "        elif token.pos_ == 'VERB': \n",
    "            str_token = lemmatiser.lemmatize(str_token, 'v')\n",
    "            new_token = insert_module(str_token, 'verb')\n",
    "\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            new_token = insert_module(str_token, 'adj')\n",
    "        \n",
    "        ################\n",
    "        ### REAL INSERT\n",
    "        #print('new token ---> ', new_token)\n",
    "        \n",
    "        pre_token_pos = token.pos_\n",
    "        \n",
    "        if new_token == 'none': # insert된 것이 하나도 없으면 pass.\n",
    "            continue # pass\n",
    "        else:\n",
    "            insert_on = True # insert 한 번 이상 되었음.\n",
    "            list_sent_raw.insert(i+add_idx, new_token)\n",
    "            list_sent_label.insert(i+add_idx, 'O')\n",
    "            filtered_list.insert(i+add_idx, -1)\n",
    "            add_idx += 1\n",
    "        \n",
    "        #print('\\n')\n",
    "        \n",
    "    \n",
    "    #print(' '.join(list_sent_raw))\n",
    "    #print(' '.join(list_sent_label))\n",
    "    #print('\\n')\n",
    "    ### Append new sent list\n",
    "    if insert_on == True:\n",
    "        insert_sent_cnt += 1\n",
    "        new_sent_list.append(list_sent_raw)\n",
    "        new_label_list.append(list_sent_label)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(new_sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path_write = 'data_logic_warehouse/type_2/insert_'+str(len(new_sent_list))+'.txt'\n",
    "\n",
    "# with open(path_write, 'w', encoding='UTF-8') as txt:\n",
    "#     for i, new_token_list in enumerate(new_sent_list):\n",
    "#         for j, token in enumerate(new_token_list):\n",
    "#             txt.write(new_sent_list[i][j]+' '+'NNP'+' '+'B-NP'+' '+new_label_list[i][j])\n",
    "#             txt.write('\\n')\n",
    "#         txt.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예외처리\n",
    "\n",
    "a bit와 같이 두 글자로 이뤄진 것을 예외처리 할 필요가 있다. 여기서 앞 글자만 따 similarity를 측정하면 높게 나온다. a는 어디서나 자주 등장하기 때문에... 따라서 외자는 제외한다는 식으로 예외처리가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
