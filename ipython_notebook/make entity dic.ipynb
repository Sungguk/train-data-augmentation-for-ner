{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ace2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_handling_for_heuristic import *\n",
    "\n",
    "ace_path = 'entity_info/ace_entity.txt'\n",
    "#read_path2 = 'valid.txt'\n",
    "\n",
    "a1, a2 = load_conll2003(ace_path)\n",
    "#b1, b2 = load_conll2003(read_path2)\n",
    "\n",
    "raw_data = a1 #+ b1\n",
    "label_data = a2 #+ b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PER_ace = []\n",
    "LOC_ace = []\n",
    "ORG_ace = []\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "path = os.getcwd() + '\\\\data\\\\ace2005'\n",
    "ace_entity_path = os.getcwd() + '\\\\entity_info\\\\ace_entity.txt'\n",
    "\n",
    "#w_file = open(ace_entity_path, 'w')\n",
    "for dirName, subdirList, fileList in os.walk(path):\n",
    "    for fname in fileList:\n",
    "        if fname.split('.')[-1] == 'conll':\n",
    "            full_path = dirName + '\\\\' + fname\n",
    "            #print(full_path)\n",
    "            file = open(full_path, 'r')\n",
    "            line_temp = file.readlines()\n",
    "            \n",
    "            #######################################\n",
    "            entity_temp = ''\n",
    "            for i, line in enumerate(line_temp):\n",
    "\n",
    "                if line.split('\\t')[-1].split('\\n')[0].split('_')[-1] == 'NAM':\n",
    "                    \n",
    "                    #print(line.split())\n",
    "                    cur_entity = line.split()[0]                   \n",
    "                    \n",
    "\n",
    "                    if line.split('\\t')[-1].split('\\n')[0].split('_')[1] == 'PER':\n",
    "                        #print(i, line.split())\n",
    "                        \n",
    "                        if i == len(line_temp)-1: # 맨 마지막\n",
    "                            PER_ace.append(entity_temp)\n",
    "\n",
    "                        else:\n",
    "                            \n",
    "                            if line.split()[-1].split('_')[0] == 'B':                              \n",
    "                                \n",
    "                                if line_temp[i+1].split()[-1].split('_')[0] != 'I':\n",
    "                                    entity_temp = cur_entity\n",
    "                                    #print(entity_temp)\n",
    "                                    PER_ace.append(entity_temp)\n",
    "                                    entity_temp = ''\n",
    "                                if line_temp[i+1].split()[-1].split('_')[0] == 'I':\n",
    "                                    entity_temp = cur_entity\n",
    "                                    #print(entity_temp)\n",
    "                            elif line.split()[-1].split('_')[0] == 'I':                         \n",
    "                                if line_temp[i+1].split()[-1].split('_')[0] != 'I':\n",
    "                                    #print(entity_temp)\n",
    "                                    entity_temp += ' '\n",
    "                                    entity_temp += cur_entity\n",
    "                                    #print(entity_temp)\n",
    "                                    PER_ace.append(entity_temp)\n",
    "                                    entity_temp = ''                                    \n",
    "                                    \n",
    "                                if line_temp[i+1].split()[-1].split('_')[0] == 'I':\n",
    "                                    entity_temp += ' '\n",
    "                                    entity_temp += cur_entity\n",
    "\n",
    "#                         w_file.write(line.split()[0]+'\\t'+'NNP'+'\\t'+'B-NP'+'\\t'+label)\n",
    "#                         w_file.write('\\n')\n",
    "                        \n",
    "                    elif line.split('\\t')[-1].split('\\n')[0].split('_')[1] == 'ORG': \n",
    "            \n",
    "                        if i == len(line_temp)-1: # 맨 마지막\n",
    "                            ORG_ace.append(entity_temp)\n",
    "\n",
    "                        else:\n",
    "                            \n",
    "                            if line.split()[-1].split('_')[0] == 'B':                              \n",
    "                                \n",
    "                                if line_temp[i+1].split()[-1].split('_')[0] != 'I':\n",
    "                                    entity_temp = cur_entity\n",
    "                                    #print(entity_temp)\n",
    "                                    ORG_ace.append(entity_temp)\n",
    "                                    entity_temp = ''\n",
    "                                if line_temp[i+1].split()[-1].split('_')[0] == 'I':\n",
    "                                    entity_temp = cur_entity\n",
    "                                    #print(entity_temp)\n",
    "                            elif line.split()[-1].split('_')[0] == 'I':                         \n",
    "                                if line_temp[i+1].split()[-1].split('_')[0] != 'I':\n",
    "                                    #print(entity_temp)\n",
    "                                    entity_temp += ' '\n",
    "                                    entity_temp += cur_entity\n",
    "                                    #print(entity_temp)\n",
    "                                    ORG_ace.append(entity_temp)\n",
    "                                    entity_temp = ''                                    \n",
    "                                    \n",
    "                                if line_temp[i+1].split()[-1].split('_')[0] == 'I':\n",
    "                                    entity_temp += ' '\n",
    "                                    entity_temp += cur_entity\n",
    "            #######################################\n",
    "        \n",
    "        #w_file.write('\\n')\n",
    "        #break\n",
    "        \n",
    "        \n",
    "#w_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 2318 1658\n",
      "Condition Remove: 2235 1483\n",
      "Long-length Remove 2228 1410\n"
     ]
    }
   ],
   "source": [
    "### 맨 앞, 대문자로\n",
    "for i, _ in enumerate(PER_ace): \n",
    "    PER_ace[i] = PER_ace[i].title() \n",
    "for i, _ in enumerate(ORG_ace):\n",
    "    if len(ORG_ace[i].split())==1:\n",
    "        if len(list(ORG_ace[i]))<=3:\n",
    "            ORG_ace[i] = ORG_ace[i].upper()\n",
    "            continue  \n",
    "    ORG_ace[i] = ORG_ace[i].title()\n",
    "print('Original:',len(PER_ace), len(ORG_ace))\n",
    "    \n",
    "### ), -, _, \", ( 또는 숫자 있으면 그냥 삭제\n",
    "def remELE_from1DList(list1D, condition):\n",
    "    for i, _ in enumerate(list1D):\n",
    "        for char in list(list1D[i]):\n",
    "            if char in condition:\n",
    "                list1D[i] = 'to-be-deleted'\n",
    "    return [ele for ele in list1D if ele != 'to-be-deleted']\n",
    "condition = [\"'\",')', '(', '-', '_', '\"', '1', '2','3','4','5','6','7','8','9','0']\n",
    "PER_ace = remELE_from1DList(PER_ace, condition)\n",
    "ORG_ace = remELE_from1DList(ORG_ace, condition)\n",
    "print('Condition Remove:', len(PER_ace), len(ORG_ace))\n",
    "\n",
    "### 중복 삭제\n",
    "def remove_duplicate(list):\n",
    "    pure_list = []\n",
    "    for x in list:\n",
    "        if not x in pure_list:\n",
    "            pure_list.append(x)\n",
    "    return pure_list\n",
    "#PER_ace = remove_duplicate(PER_ace)\n",
    "#ORG_ace = remove_duplicate(ORG_ace)\n",
    "#print('Duplicate Remove',len(PER_ace), len(ORG_ace))\n",
    "\n",
    "### 길이가 긴 애들 삭제\n",
    "def remove_long(list):\n",
    "    for i, _ in enumerate(list):\n",
    "        if len(list[i].split()) >= 4:\n",
    "               list[i] = 'to-be-deleted'\n",
    "    return [ele for ele in list if ele != 'to-be-deleted']\n",
    "PER_ace = remove_long(PER_ace)\n",
    "ORG_ace = remove_long(ORG_ace)        \n",
    "print('Long-length Remove',len(PER_ace), len(ORG_ace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_path1 = 'train.txt'\n",
    "#read_path2 = 'valid.txt'\n",
    "\n",
    "a1, a2 = load_conll2003(read_path1)\n",
    "#b1, b2 = load_conll2003(read_path2)\n",
    "\n",
    "raw_data = a1 #+ b1\n",
    "label_data = a2 #+ b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract entities from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PER_data = []\n",
    "LOC_data = []\n",
    "ORG_data = []\n",
    "MISC_data = []\n",
    "for i, sent in enumerate(label_data):\n",
    "    sent_label = label_data[i].split()\n",
    "    sent_raw = raw_data[i].split()\n",
    "    \n",
    "    # 초기화\n",
    "    pre_label = 'O'\n",
    "    entity = ''\n",
    "    \n",
    "    for j, token in enumerate(sent_label):\n",
    "        \n",
    "        if sent_label[j].split('-')[-1] == 'PER':\n",
    "\n",
    "            if j==len(sent_label)-1:\n",
    "                entity += sent_raw[j]\n",
    "                PER_data.append(entity)\n",
    "                entity = ''\n",
    "            else: # j != len(sent_label)-1\n",
    "                if sent_label[j+1].split('-')[-1] == 'PER':\n",
    "                    entity += sent_raw[j]\n",
    "                    entity += ' '\n",
    "                else:\n",
    "                    entity += sent_raw[j]\n",
    "                    PER_data.append(entity)\n",
    "                    entity = ''\n",
    "                \n",
    "        elif sent_label[j].split('-')[-1] == 'LOC':\n",
    "\n",
    "            if j==len(sent_label)-1:\n",
    "                entity += sent_raw[j]\n",
    "                LOC_data.append(entity)\n",
    "                entity = ''\n",
    "            else: # j != len(sent_label)-1\n",
    "                if sent_label[j+1].split('-')[-1] == 'LOC':\n",
    "                    entity += sent_raw[j]\n",
    "                    entity += ' '\n",
    "                else:\n",
    "                    entity += sent_raw[j]\n",
    "                    LOC_data.append(entity)\n",
    "                    entity = ''\n",
    "                    \n",
    "        elif sent_label[j].split('-')[-1] == 'ORG':\n",
    "\n",
    "            if j==len(sent_label)-1:\n",
    "                entity += sent_raw[j]\n",
    "                ORG_data.append(entity)\n",
    "                entity = ''\n",
    "            else: # j != len(sent_label)-1\n",
    "                if sent_label[j+1].split('-')[-1] == 'ORG':\n",
    "                    entity += sent_raw[j]\n",
    "                    entity += ' '\n",
    "                else:\n",
    "                    entity += sent_raw[j]\n",
    "                    ORG_data.append(entity)\n",
    "                    entity = ''\n",
    "                    \n",
    "        elif sent_label[j].split('-')[-1] == 'MISC':\n",
    "\n",
    "            if j==len(sent_label)-1:\n",
    "                entity += sent_raw[j]\n",
    "                ORG_data.append(entity)\n",
    "                entity = ''\n",
    "            else: # j != len(sent_label)-1\n",
    "                if sent_label[j+1].split('-')[-1] == 'MISC':\n",
    "                    entity += sent_raw[j]\n",
    "                    entity += ' '\n",
    "                else:\n",
    "                    entity += sent_raw[j]\n",
    "                    ORG_data.append(entity)\n",
    "                    entity = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600 7129 9700\n"
     ]
    }
   ],
   "source": [
    "PER_total = PER_data #+ PER_ace\n",
    "LOC_total = LOC_data\n",
    "ORG_total = ORG_data #+ ORG_ace\n",
    "print(len(PER_total), len(LOC_total), len(ORG_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove overlapped entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MISC와 겹치는 것들은 리스트에서 제외하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6559 6414 8895\n"
     ]
    }
   ],
   "source": [
    "PER_temp = PER_total[:]\n",
    "LOC_temp = LOC_total[:]\n",
    "ORG_temp = ORG_total[:]\n",
    "\n",
    "for i, _ in enumerate(PER_total):\n",
    "    if PER_total[i] in LOC_temp:\n",
    "        PER_total[i] = 'to-be-deleted'\n",
    "    elif PER_total[i] in ORG_temp:\n",
    "        PER_total[i] = 'to-be-deleted'\n",
    "\n",
    "for i, _ in enumerate(LOC_total):\n",
    "    if LOC_total[i] in ORG_temp:\n",
    "        LOC_total[i] = 'to-be-deleted'\n",
    "    elif LOC_total[i] in PER_temp:\n",
    "        LOC_total[i] = 'to-be-deleted'\n",
    "\n",
    "for i, _ in enumerate(ORG_total):\n",
    "    if ORG_total[i] in LOC_temp:\n",
    "        ORG_total[i] = 'to-be-deleted'\n",
    "    elif ORG_total[i] in PER_temp:\n",
    "        ORG_total[i] = 'to-be-deleted'\n",
    "        \n",
    "\n",
    "PER_total = [ele for ele in PER_total if ele != 'to-be-deleted']        \n",
    "LOC_total = [ele for ele in LOC_total if ele != 'to-be-deleted'] \n",
    "ORG_total = [ele for ele in ORG_total if ele != 'to-be-deleted']\n",
    "\n",
    "print(len(PER_total), len(LOC_total), len(ORG_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Remove duplicates\n",
    "# ORG_data = remove_duplicate(ORG_data)\n",
    "# PER_data = remove_duplicate(PER_data)\n",
    "# LOC_data = remove_duplicate(LOC_data)\n",
    "# print(len(ORG_data), len(PER_data), len(LOC_data))\n",
    "\n",
    "save(ORG_total, 'entity_info/ORG_data(onlyCONLL)')\n",
    "save(PER_total, 'entity_info/PER_data(onlyCONLL)')\n",
    "save(LOC_total, 'entity_info/LOC_data(onlyCONLL)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract entities from sense2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ORG_sense2vec = []\n",
    "# PER_sense2vec = []\n",
    "# LOC_sense2vec = []\n",
    "# thr = 10\n",
    "\n",
    "# # ORG\n",
    "# for entry in ORG_data:\n",
    "#     entry = entry.replace(' ', '_')\n",
    "#     try:\n",
    "#         _, query_vector = model[entry + '|' + 'ORG']\n",
    "#         result = ((model.most_similar(query_vector, n=thr))[0])\n",
    "#         result = [item for item in result if item.split('|')[-1]=='ORG']\n",
    "#         entry = entry.replace('_', ' ')\n",
    "#         for j, nbh in enumerate(result):\n",
    "#             if j==0: continue\n",
    "#             nbh = nbh.split('|')[0].replace('_', ' ')\n",
    "#             nbh = nbh.replace(\"'\", \" '\")\n",
    "#             if nbh.split()[-1] == 'of' or nbh.split()[-1] == 'for': continue\n",
    "#             if ';' in list(nbh): continue\n",
    "#             if ':' in list(nbh): continue\n",
    "#             if list(nbh.split()[0])[0].isupper() == False: continue\n",
    "#             if nbh == '76ers': continue\n",
    "#             if nbh == 'FUCK FUCK FUCK FUCK': continue\n",
    "#             ORG_sense2vec.append((entry, nbh))\n",
    "#     except(KeyError):\n",
    "#         pass\n",
    "# # PER\n",
    "# for entry in PER_data:\n",
    "#     entry = entry.replace(' ', '_')\n",
    "#     try:\n",
    "#         _, query_vector = model[entry + '|' + 'PERSON']\n",
    "#         result = ((model.most_similar(query_vector, n=thr))[0])\n",
    "#         result = [item for item in result if item.split('|')[-1]=='PERSON']\n",
    "#         entry = entry.replace('_', ' ')\n",
    "#         for j, nbh in enumerate(result):\n",
    "#             if j==0: continue\n",
    "#             nbh = nbh.split('|')[0].replace('_', ' ')\n",
    "#             nbh = nbh.replace(\"'\", \" '\")\n",
    "#             if nbh.split()[-1] == 'of' or nbh.split()[-1] == 'for': continue\n",
    "#             if ';' in list(nbh): continue\n",
    "#             if ':' in list(nbh): continue\n",
    "#             if list(nbh.split()[0])[0].isupper() == False: continue\n",
    "#             if nbh == '76ers': continue\n",
    "#             if nbh == 'FUCK FUCK FUCK FUCK': continue\n",
    "#             PER_sense2vec.append((entry, nbh))\n",
    "#     except(KeyError):\n",
    "#         pass\n",
    "# # LOC\n",
    "# for entry in LOC_data:\n",
    "#     entry = entry.replace(' ', '_')\n",
    "#     try:\n",
    "#         _, query_vector = model[entry + '|' + 'ORG']\n",
    "#         result = ((model.most_similar(query_vector, n=thr))[0])\n",
    "#         result = [item for item in result if item.split('|')[-1]=='ORG']\n",
    "#         entry = entry.replace('_', ' ')\n",
    "#         for j, nbh in enumerate(result):\n",
    "#             if j==0: continue\n",
    "#             nbh = nbh.split('|')[0].replace('_', ' ')\n",
    "#             nbh = nbh.replace(\"'\", \" '\")\n",
    "#             if nbh.split()[-1] == 'of' or nbh.split()[-1] == 'for': continue\n",
    "#             if ';' in list(nbh): continue\n",
    "#             if ':' in list(nbh): continue\n",
    "#             if list(nbh.split()[0])[0].isupper() == False: continue\n",
    "#             if nbh == '76ers': continue\n",
    "#             if nbh == 'FUCK FUCK FUCK FUCK': continue\n",
    "#             LOC_sense2vec.append((entry, nbh))\n",
    "#     except(KeyError):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ORG_sense2vec_unique = []\n",
    "# ORG_temp = []\n",
    "# for x in ORG_sense2vec:\n",
    "#     if x[0] not in ORG_temp:\n",
    "#         ORG_temp.append(x[0])\n",
    "#         ORG_sense2vec_unique.append(x)\n",
    "        \n",
    "# PER_sense2vec_unique = []\n",
    "# PER_temp = []\n",
    "# for x in PER_sense2vec:\n",
    "#     if x[0] not in PER_temp:\n",
    "#         PER_temp.append(x[0])\n",
    "#         PER_sense2vec_unique.append(x)\n",
    "        \n",
    "# LOC_sense2vec_unique = []\n",
    "# LOC_temp = []\n",
    "# for x in LOC_sense2vec:\n",
    "#     if x[0] not in LOC_temp:\n",
    "#         LOC_temp.append(x[0])\n",
    "#         LOC_sense2vec_unique.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save only from sense2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(ORG_sense2vec), len(PER_sense2vec), len(LOC_sense2vec))\n",
    "# save(ORG_sense2vec, 'entity_info/ORG_sense2vec')\n",
    "# save(PER_sense2vec, 'entity_info/PER_sense2vec')\n",
    "# save(LOC_sense2vec, 'entity_info/LOC_sense2vec')\n",
    "\n",
    "# print(len(ORG_sense2vec_unique), len(PER_sense2vec_unique), len(LOC_sense2vec_unique))\n",
    "# save(ORG_sense2vec_unique, 'entity_info/ORG_sense2vec_unique')\n",
    "# save(PER_sense2vec_unique, 'entity_info/PER_sense2vec_unique')\n",
    "# save(LOC_sense2vec_unique, 'entity_info/LOC_sense2vec_unique')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
