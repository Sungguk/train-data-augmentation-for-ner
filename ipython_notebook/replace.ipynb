{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주의해야 될점은 word embedding의 종류를 잘 선택하는 것도 중요하다. 예를 들어 모델에서 glove를 사용한다면, 여기서도 glove를 사용하는 것이 맞지 않나 생각한다. (word embedding 종류에 따라 similairty 수치가 차이가 날 것이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy\n",
    "import sense2vec\n",
    "import random\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "nlp = spacy.load('en')\n",
    "model = sense2vec.load()\n",
    "\n",
    "########################\n",
    "### Load and Prepare Dataset\n",
    "from data_handling_for_heuristic import *\n",
    "#read_path = 'data/conll2003/eng.train'\n",
    "read_path = 'train.txt'\n",
    "raw_data, label_data = load_conll2003(read_path)\n",
    "\n",
    "########################\n",
    "### 완벽한 문장만 필터링\n",
    "sent_raw = []\n",
    "sent_label = []\n",
    "cnt = 0\n",
    "for i, row in enumerate(raw_data):\n",
    "    row_nlp = nlp(row)\n",
    "\n",
    "#     if len(raw_data[i].split()) >= 2: # 길이가 최소 2이상인 문장들마나 선별하자. (ex. .만 있는 문장도 있다)\n",
    "#         temp = []\n",
    "#         for token in row_nlp:\n",
    "#             temp.append(token.pos_) # 각 token의 pos 저장\n",
    "#         if 'VERB' in temp: # 문장에 최소 1개이상의 verb가 있어야 한다.\n",
    "#             if len([x for x in label_data[i].split() if x != 'O']) != 0: # 엔티티가 하나도 없으면 안된다.\n",
    "#                 sent_raw.append(raw_data[i])\n",
    "#                 sent_label.append(label_data[i])  \n",
    "    \n",
    "    if raw_data[i][-1] == '.' or raw_data[i][-1] == '\"': # 마지막에 쉼표가 있는 문장들만 선별하자.\n",
    "        if len(raw_data[i].split()) >= 2: # 길이가 최소 2이상인 문장들마나 선별하자. (ex. .만 있는 문장도 있다)\n",
    "            temp = []\n",
    "            for token in row_nlp:\n",
    "                temp.append(token.pos_) # 각 token의 pos 저장\n",
    "            if 'VERB' in temp: # 문장에 최소 1개이상의 verb가 있어야 한다.\n",
    "                if len([x for x in label_data[i].split() if x != 'O']) != 0: # 엔티티가 하나도 없으면 안된다.\n",
    "                    sent_raw.append(raw_data[i])\n",
    "                    sent_label.append(label_data[i]) \n",
    "#                 else:\n",
    "#                     cnt += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 전체 문장: **14987**\n",
    "   * 엔티티가 하나도 없는 문장: **3855**\n",
    "      * 완전한 문장 & 엔티티가 하나도 없는 문장: **1339**\n",
    "      * 불완전한 문장 & 엔티티가 하나도 없는 문장: 3855 - 1339 = **2516**\n",
    "   * 엔티티가 하나라도 있는 문장: 14987 - 3855 = **11132**\n",
    "      * 완전한 문장 & 엔티티가 하나라도 있는 문장: **5143**\n",
    "      * 불완전한 문장 & 엔티티가 하나라도 있는 문장: 11132 - 5143 = **5989**\n",
    "      \n",
    "<br>\n",
    "전체 문자에서 우리 데이터 강화 모델에 들어갈 수 있는 문장 개수는 14987개 중에서 고작 5143개밖에 안된다. 전체에서 34.31%밖에 되질 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent_raw: 문장 단위 + 정상적인 문장 텍스트 <br>\n",
    "sent_label: 문장 단위 + 정상적인 문장 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ### Store good sentence data\n",
    "# path_write = 'good_sent.txt'\n",
    "# with open(path_write, 'w', encoding='UTF-8') as txt:\n",
    "#     for i, new_sent_raw_list in enumerate(sent_raw):\n",
    "#         splited_sent = sent_raw[i].split()\n",
    "#         splited_label = sent_label[i].split()\n",
    "#         for j, token in enumerate(splited_sent):\n",
    "#             txt.write(splited_sent[j]+' '+'NNP'+' '+'B-NP'+' '+splited_label[j])\n",
    "#             txt.write('\\n')\n",
    "#         txt.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-10-28_13:30_'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 임시\n",
    "# from data_handling_for_heuristic import *\n",
    "# #read_path = 'data/conll2003/eng.train'\n",
    "# read_path = 'data_logic_warehouse/type_1/delete_1077.txt'\n",
    "# #read_path = 'train.txt'\n",
    "# sent_raw, sent_label = load_conll2003(read_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_with_word2vec(word, pos):\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)\n",
    "    num_upbound = 10\n",
    "    nlp_word = nlp.vocab[word]\n",
    "\n",
    "    ### similarity가 비슷한 단어들 추출\n",
    "    queries = [w for w in nlp_word.vocab if w.is_lower == nlp_word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: nlp_word.similarity(w), reverse=True)\n",
    "    #print([w.lower_ for w in by_similarity[:num_upbound]])\n",
    "    cand_word_list = [w.lower_ for w in by_similarity[:num_upbound]] # some candidate words\n",
    "    #print(cand_word_list)\n",
    "    \n",
    "    ### 후보군들 중에서 어떤 것을 교체어로 선정할 것인가?\n",
    "    # stemming과 lemmatization을 모두 사용하자.\n",
    "    source_stem = stemmer.stem((word))\n",
    "    source_lemma = lemmatiser.lemmatize((word), pos)\n",
    "    \n",
    "    for i, token in enumerate(cand_word_list):\n",
    "        #print(source_stem, stemmer.stem(str(token)), source_lemma, lemmatiser.lemmatize(str(token), pos))\n",
    "        if source_stem == stemmer.stem((token)):\n",
    "            cand_word_list[i] = 'to-be-deleted'\n",
    "        if source_lemma == lemmatiser.lemmatize((token), pos):\n",
    "            cand_word_list[i] = 'to-be-deleted'\n",
    "    cand_word_list = [x for x in cand_word_list if x != 'to-be-deleted'] # 한번에 삭제\n",
    "    \n",
    "    print(cand_word_list[0], nlp_word.similarity(nlp.vocab[cand_word_list[0]]))\n",
    "\n",
    "    #print(cand_word_list)\n",
    "    # 다 삭제하고 empty면 그냥 교체하지 말자. (ex. are: ['are', 'were', 'werent', \"'re\", 'is'])\n",
    "    if len(cand_word_list)==0:\n",
    "        return word\n",
    "    else:\n",
    "        return cand_word_list[0] # 후보군 중에서 가장 맨 앞에 있는 단어 선택\n",
    "\n",
    "\n",
    "def replace_with_sense2vec(word, pos):\n",
    "    global empty_num_sense2vec\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)\n",
    "    num_upbound = 5\n",
    "    try:\n",
    "        freq, query_vector = model[word+'|'+pos]\n",
    "    except KeyError:\n",
    "        empty_num_sense2vec += 1\n",
    "        return 'sense2vec is empty!!!'\n",
    "    return model.most_similar(query_vector, n=num_upbound)\n",
    "\n",
    "\n",
    "def replace_with_wordnet(word, pos):\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)   \n",
    "        \n",
    "    dog_synsets = wn.synsets(word,pos) # wordnet은 이렇게 POS도 인자로 넣을 수 있다\n",
    "    for i, syn in enumerate(dog_synsets):\n",
    "        print('%d. %s' % (i, syn.name()))\n",
    "        print('alternative names (lemmas): \"%s\"' % '\", \"'.join(syn.lemma_names()))\n",
    "        \n",
    "        #print('definition: \"%s\"' % syn.definition())\n",
    "        #if syn.examples():\n",
    "        #    print('example usage: \"%s\"' % '\", \"'.join(syn.examples()))\n",
    "        \n",
    "def preprocessing_for_spacy(sent_raw):\n",
    "    \n",
    "    ### 문장 string 전체 단위\n",
    "    sent_raw = sent_raw.replace(\"'ve\", 'have')\n",
    "    \n",
    "    ### 문장 내에 있는 token 단위\n",
    "    # nlp output과 sent_raw의 길이가 같도록 하기 위해 전처리 실시\n",
    "    # ex. nlp()를 거치면, EU-wide와 같은 단어는 EU, -, wide로 3개로 분리된다. \n",
    "    splited_sent_raw = sent_raw.split()\n",
    "    for i, token in enumerate(splited_sent_raw):\n",
    "        splited_token = token.split('-')\n",
    "        if not splited_token == 1: # 일반 단어들이 아니라면, result. 'EU-wide', '--'\n",
    "            if not (splited_token[0] == '' or splited_token[-1] =='km'): # '-'으로만 이뤄진 단어가 아니라면, result.'EU-wide'\n",
    "                filtered_token = [x for x in splited_token if x != '-'] # '-'를 list에서 삭제 result.['EU', 'wide']\n",
    "                merged_token = ''.join(filtered_token)\n",
    "                splited_sent_raw[i] = merged_token\n",
    "    \n",
    "    for i, token in enumerate(splited_sent_raw):\n",
    "        if not len(token) == 1: # . 한개만 있는 token은 제외\n",
    "            splited_sent_raw[i] = splited_sent_raw[i].replace('.', '')\n",
    "            splited_sent_raw[i] = splited_sent_raw[i].replace('$', '')\n",
    "        if token == 'cannot':\n",
    "            splited_sent_raw[i] = \"can\"\n",
    "        if token == 'dont':\n",
    "            splited_sent_raw[i] = \"do\"\n",
    "        if token == \"'re\":\n",
    "            splited_sent_raw[i] = \"are\"\n",
    "        if token == \"**\":\n",
    "            splited_sent_raw[i] = \"*\"        \n",
    "        if token == \"...\" or token == \"..\" or token == \"....\":\n",
    "            splited_sent_raw[i] = \".\"         \n",
    "        if token == \"'m\":\n",
    "            splited_sent_raw[i] = \"am\"         \n",
    "        if token == \"'ll\":\n",
    "            splited_sent_raw[i] = \"will\" \n",
    "        if token == \"'d\":\n",
    "            splited_sent_raw[i] = \"would\"\n",
    "            \n",
    "        # very specific problem of this task    \n",
    "        if token == \"*Note\":\n",
    "            splited_sent_raw[i] = \"Note\"   \n",
    "        if token == \"*Name\":\n",
    "            splited_sent_raw[i] = \"Name\" \n",
    "        if token == 'km':\n",
    "            splited_sent_raw[i] = \"miles\"             \n",
    "        if token == '237km':\n",
    "            splited_sent_raw[i] = \"237-km\"             \n",
    "    return ' '.join(splited_sent_raw)\n",
    "\n",
    "\n",
    "def wordnet(word, pos):\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)   \n",
    "    synsets = wn.synsets(word, pos)\n",
    "    list_synsets = []\n",
    "    for i, syn in enumerate(synsets):\n",
    "        #print('%d. %s' % (i, syn.name()))\n",
    "        li = (syn.lemma_names())\n",
    "        list_synsets += (li)\n",
    "        for hyper_syn in syn.hypernyms():\n",
    "            list_synsets += (hyper_syn.lemma_names())\n",
    "        for hypo_syn in syn.hyponyms():\n",
    "            list_synsets += (hypo_syn.lemma_names())\n",
    "        for holo_syn in syn.part_holonyms():\n",
    "            list_synsets += (holo_syn.lemma_names())\n",
    "        for mero_syn in syn.part_meronyms():\n",
    "            list_synsets += (mero_syn.lemma_names())\n",
    "\n",
    "    # 똑같은 stem 제거\n",
    "    source_stem = stemmer.stem(word)\n",
    "    source_lemma = lemmatiser.lemmatize(word, pos)\n",
    "    for i, token in enumerate(list_synsets):\n",
    "        #print(source_stem, stemmer.stem(str(token)), source_lemma, lemmatiser.lemmatize(str(token), pos))\n",
    "        if source_stem == stemmer.stem(token):\n",
    "            list_synsets[i] = 'to-be-deleted'\n",
    "        if source_lemma == lemmatiser.lemmatize(token, pos):\n",
    "            list_synsets[i] = 'to-be-deleted'\n",
    "        if len(token.split('_')) != 1: # 'bill_of_exchange'와 같이 명사구 또는 복합명사일 경우... 삭제...\n",
    "            list_synsets[i] = 'to-be-deleted'\n",
    "    list_synsets = [x for x in list_synsets if x != 'to-be-deleted'] # 한번에 삭제      \n",
    "    return list_synsets\n",
    "\n",
    "\n",
    "def word2vec(word, pos):\n",
    "    thr = 0.7\n",
    "    thr = thr / 2.0\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)\n",
    "    num_upbound = 5\n",
    "    nlp_word = nlp.vocab[word]\n",
    "\n",
    "    ### similarity가 비슷한 단어들 추출\n",
    "    queries = [w for w in nlp_word.vocab if w.is_lower == nlp_word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: nlp_word.similarity(w), reverse=True)\n",
    "    #print([w.lower_ for w in by_similarity[:num_upbound]])\n",
    "    cand_word_list = [w.lower_ for w in by_similarity[:num_upbound]] # some candidate words\n",
    "    #print(cand_word_list)\n",
    "    \n",
    "    ### 후보군들 중에서 어떤 것을 교체어로 선정할 것인가?\n",
    "    # stemming과 lemmatization을 모두 사용하자.\n",
    "    source_stem = stemmer.stem(word)\n",
    "    source_lemma = lemmatiser.lemmatize(word, pos)\n",
    "    \n",
    "    for i, token in enumerate(cand_word_list):\n",
    "        #print(source_stem, stemmer.stem(str(token)), source_lemma, lemmatiser.lemmatize(str(token), pos))\n",
    "        if source_stem == stemmer.stem(token):\n",
    "            cand_word_list[i] = 'to-be-deleted'\n",
    "        if source_lemma == lemmatiser.lemmatize(token, pos):\n",
    "            cand_word_list[i] = 'to-be-deleted'\n",
    "    cand_word_list = [x for x in cand_word_list if x != 'to-be-deleted'] # 한번에 삭제\n",
    "    \n",
    "    #print(cand_word_list)\n",
    "    #print('\\n')\n",
    "    \n",
    "    if len(cand_word_list) == 0:\n",
    "        return False\n",
    "    if nlp_word.similarity(nlp.vocab[cand_word_list[0]]) > thr: # 70%로 비슷하면 \n",
    "        #return cand_word_list[0] # 최상위 1개 선택\n",
    "        return random.choice(cand_word_list) # 랜덤으로 선택\n",
    "        \n",
    "    else:\n",
    "        list_wordnet = wordnet(word, pos)\n",
    "        cand_word_list_filtered_from_wornet = [v for v in cand_word_list if v in list_wordnet]\n",
    "        if len(cand_word_list_filtered_from_wornet)==0:\n",
    "            if len(list_wordnet)==0:\n",
    "                return False\n",
    "            else:\n",
    "                return list_wordnet[0]\n",
    "        else:\n",
    "            #print(cand_word_list_filtered_from_wornet)\n",
    "            #print('\\n')\n",
    "            return random.choice(cand_word_list_filtered_from_wornet)\n",
    "            #return cand_word_list_filtered_from_wornet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_algorithm(list_sent_raw, list_rep, pos):\n",
    "    bit = False\n",
    "    new_list_sent_raw = list_sent_raw[:]\n",
    "    for token, _, idx in list_rep:\n",
    "        \n",
    "        result = word2vec(token, pos)\n",
    "        if result != False: # 실패하지 않은 경우에만...\n",
    "            bit = True\n",
    "            new_list_sent_raw[idx] = result\n",
    "        \n",
    "    return new_list_sent_raw, bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def window_filtering(sent_label, window_size=3):\n",
    "    list_sent_raw = sent_raw.split()\n",
    "    list_sent_label = sent_label.split()\n",
    "    new_list = [0] * len(list_sent_raw)\n",
    "    \n",
    "    ### 순방향\n",
    "    for i, _ in enumerate(list_sent_label):\n",
    "        temp = list_sent_label[i:i+1+window_size]\n",
    "        #print(temp)\n",
    "        \n",
    "        tkn = False\n",
    "        for each in temp:\n",
    "            if each != 'O':\n",
    "                tkn = True\n",
    "        if tkn == True:\n",
    "            new_list[i] = 1\n",
    "             \n",
    "    ### 역방향\n",
    "    for i  in range(len(list_sent_label)-1, -1, -1):\n",
    "        idx = i-window_size\n",
    "        if idx < 0:\n",
    "            idx = 0\n",
    "        temp = list_sent_label[idx:i]\n",
    "        \n",
    "        tkn = False\n",
    "        for each in temp:\n",
    "            if each != 'O':\n",
    "                tkn = True\n",
    "        if tkn == True:\n",
    "            new_list[i] = 1      \n",
    "        \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-03293f0236f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindexx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m71\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msent_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_label' is not defined"
     ]
    }
   ],
   "source": [
    "indexx = 71\n",
    "sent_label[indexx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_filtering(sent_raw[indexx], sent_label[indexx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generation_via_replace(sent_raw, sent_label, alpha):\n",
    "\n",
    "    #############\n",
    "    \"\"\" 전처리 \"\"\"\n",
    "    #############\n",
    "    # 동사, 명사는 context 정보의 핵심이다 모든 언어가 공통적으로 가지고 있는 특징.  \n",
    "    #print(sent_raw)\n",
    "    list_sent_raw = sent_raw.split()\n",
    "    sent_raw = preprocessing_for_spacy(sent_raw)\n",
    "    sent_nlp = nlp(sent_raw)\n",
    "    noun_sorted = []\n",
    "    verb_sorted = []\n",
    " \n",
    "    ### assert 에러 났을 때, 원인 찾기 위해 print 실시\n",
    "    if not len(sent_nlp) == len(sent_label.split()):\n",
    "        print(len(sent_raw.split()), len(sent_nlp), len(sent_label.split()))\n",
    "        print(sent_raw)\n",
    "        print(sent_nlp)\n",
    "        for i, token in enumerate(sent_nlp):\n",
    "            print(token, sent_raw.split()[i], sent_label.split()[i])\n",
    "    assert(len(sent_nlp) == len(sent_label.split()))\n",
    "    \n",
    "    ###################################\n",
    "    \"\"\" 조건에 맞는 token들을 불러온다 \"\"\"\n",
    "    ###################################\n",
    "    for i, token in enumerate(sent_nlp):\n",
    "        \n",
    "        \n",
    "        ### 제외되는 token들..\n",
    "        if not sent_label.split()[i] == 'O': # 엔티티가 아닌 token은 제외\n",
    "            continue\n",
    "        is_uppercase_letter = True in map(lambda l: l.isupper(), str(token))\n",
    "        if i != 0 and is_uppercase_letter == True: # 첫 번째 글자가 아니고 대문자가 하나라도 있는 token은 제외 (ex. Thursday와 같은 시간고유명사. 크게 의미적이지 않음.)\n",
    "            continue\n",
    "        if lemmatiser.lemmatize(str(token), 'v') == 'be': # be동사 lemma를 가지는 token은 제외\n",
    "            continue\n",
    "        if lemmatiser.lemmatize(str(token), 'v') == 'have': # have 동사 lemma를 가지는 token은 제외, (have 동사의 의미가 광범위하게 쓰이기도 하고, have가 분사형으로 사용되기도 하기 때문이다)\n",
    "            continue\n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "        ### 동사, 명사만 추출\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            pair = (sent_nlp[i], len(list(token.subtree)), i)\n",
    "            noun_sorted.append(pair) # 명사\n",
    "               \n",
    "        elif token.pos_ == 'VERB':\n",
    "            pair = (sent_nlp[i], len(list(token.subtree)), i)\n",
    "            verb_sorted.append(pair) # 동사               \n",
    "    \n",
    "    \n",
    "    ### dependency 크기로 정렬\n",
    "    noun_sorted.sort(key=lambda x: x[1], reverse=True)\n",
    "    verb_sorted.sort(key=lambda x: x[1], reverse=True)\n",
    "#     print((noun_sorted))\n",
    "#     print((verb_sorted))\n",
    "    if len(noun_sorted)==0 and len(verb_sorted)==0:\n",
    "        return False # 교체 후보 token이 아예없다면, 새로운 문장을 만들 수 없다.\n",
    "\n",
    "    ### thr 정하기\n",
    "    noun_thr = int(len(noun_sorted) * alpha)\n",
    "    verb_thr = int(len(verb_sorted) * alpha)\n",
    "    if noun_thr == 0:\n",
    "        noun_thr = 1 # 최소값은 1로 유지\n",
    "    if verb_thr == 0:\n",
    "        verb_thr = 1 # 최소값은 1로 유지\n",
    "    noun_rep = noun_sorted[:noun_thr] # token list to be replaced\n",
    "    verb_rep = verb_sorted[:verb_thr] # token list to be replaced\n",
    "    \n",
    "    #print(noun_rep)\n",
    "    #print(verb_rep)\n",
    "    #print(list_sent_raw)\n",
    "    \n",
    "    if len(noun_sorted) != 0: # 각자 0이 될 수도 있으니...\n",
    "        \n",
    "        list_sent_raw, bit_n = replace_algorithm(list_sent_raw, noun_rep, 'n')\n",
    "        \n",
    "        #print(list_sent_raw)\n",
    "        \n",
    "        #replaced_noun = (noun_sorted[0][0], noun_sorted[0][2]) # (token, index) pair \n",
    "       \n",
    "        # 명사\n",
    "        #print('<<< ', replaced_noun[0], '  >>>')\n",
    "        #target_noun = replace_with_word2vec(replaced_noun[0], 'n')\n",
    "        #print(replace_with_sense2vec(replaced_noun[0], 'NOUN'))\n",
    "        #print(replace_with_wordnet(replaced_noun[0], 'n'))\n",
    "        #print('replaced_noun: ', replaced_noun, ', target_noun: ', target_noun)\n",
    "        #list_sent_raw[replaced_noun[1]] = target_noun # 교체\n",
    "    else:\n",
    "        bit_n = False\n",
    "        \n",
    "    \n",
    "    if len(verb_sorted) != 0: # 각자 0이 될 수도 있으니...\n",
    "        \n",
    "        list_sent_raw, bit_v = replace_algorithm(list_sent_raw, verb_rep, 'v')\n",
    "        #print(list_sent_raw)\n",
    "        #replaced_verb = (verb_sorted[0][0], verb_sorted[0][2])\n",
    "        # 동사\n",
    "        #print('<<< ', replaced_verb[0], '  >>>')\n",
    "        #target_verb = replace_with_word2vec(replaced_verb[0], 'v')\n",
    "        #print(replace_with_sense2vec(replaced_verb[0], 'VERB'))\n",
    "        #print(replace_with_wordnet(replaced_verb[0], 'v'))   \n",
    "        #print('replaced_verb: ', replaced_verb, ', target_verb: ', target_verb)\n",
    "        #list_sent_raw[replaced_verb[1]] = target_verb # 교체\n",
    "    else:\n",
    "        bit_v = False\n",
    "    \n",
    "    if bit_n == False and bit_v == False:\n",
    "        return False\n",
    "    else:    \n",
    "        #print(' '.join(list_sent_raw))\n",
    "        return ' '.join(list_sent_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt_replace_word = 0\n",
    "new_sent_raw_list = []\n",
    "new_sent_label_list = [] \n",
    "idx = 0\n",
    "\n",
    "for i, row in enumerate(sent_raw):\n",
    "\n",
    "    #if random.choice([True, False, False, False]) == False:\n",
    "    #    continue\n",
    "    \n",
    "    \n",
    "    #print(sent_raw[i])\n",
    "    #print(sent_label[i])\n",
    "    \n",
    "    #alpha_list = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    #alpha = random.choice(alpha_list)\n",
    "    alpha = 0.9\n",
    "    result = generation_via_replace(sent_raw[i], sent_label[i], alpha)\n",
    "    \n",
    "    if result == False: # 교체어가 없었는 경우\n",
    "        continue\n",
    "    else:\n",
    "        cnt_replace_word += 1\n",
    "        new_sent_raw_list.append(result)\n",
    "        new_sent_label_list.append(sent_label[i]) # 라벨은 동일하다.\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(', \\n')\n",
    "#         idx += 1\n",
    "#     #     print('-------------------------------------------------------------------------')\n",
    "#         if idx==10:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5119"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_replace_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store new sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_write = 'data_logic_warehouse/type_2/replace_1_'+str(len(new_sent_raw_list))+'.txt'\n",
    "\n",
    "with open(path_write, 'w', encoding='UTF-8') as txt:\n",
    "    \n",
    "    for i, _ in enumerate(new_sent_raw_list):\n",
    "        splited_sent = new_sent_raw_list[i].split()\n",
    "        splited_label = new_sent_label_list[i].split()\n",
    "        for j, token in enumerate(splited_sent):\n",
    "            txt.write(splited_sent[j]+' '+'NNP'+' '+'B-NP'+' '+splited_label[j])\n",
    "            txt.write('\\n')\n",
    "        txt.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "worde2vec, globe 사이에 얼만큼 차이가 존재하나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166050109258\n",
      "0.211715064563\n",
      "0.18100944888\n",
      "0.186126121673\n"
     ]
    }
   ],
   "source": [
    "def similarity(self, other):\n",
    "    self = nlp.vocab[self]\n",
    "    other = nlp.vocab[other]\n",
    "    if self.vector_norm == 0 or other.vector_norm == 0:\n",
    "        return 0.0\n",
    "    return numpy.dot(self.vector, other.vector) / (self.vector_norm * other.vector_norm)\n",
    "\n",
    "print(similarity('accumulate', 'gradually'))\n",
    "print(similarity('accumulate', 'slowly'))\n",
    "print(similarity('accumulate', 'steadily'))\n",
    "print(similarity('accumulate', 'rapidly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166244929759\n",
      "0.127519599453\n",
      "0.173745130457\n",
      "0.16867886788\n",
      "0.190444475971\n",
      "0.206913823912\n",
      "0.180141605287\n",
      "0.133837648909\n"
     ]
    }
   ],
   "source": [
    "print(similarity('accuse', 'practically'))\n",
    "print(similarity('accuse', 'virtually'))\n",
    "print(similarity('accuse', 'falsely'))\n",
    "print(similarity('accuse', 'unjustly'))\n",
    "print(similarity('accuse', 'wrongly'))\n",
    "print(similarity('accuse', 'angrily'))\n",
    "print(similarity('accuse', 'openly'))\n",
    "print(similarity('accuse', 'publicly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.176827600686\n"
     ]
    }
   ],
   "source": [
    "print(similarity('wonder', 'wide-eyed'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166050109258\n",
      "0.211715064563\n",
      "0.18100944888\n",
      "0.186126121673\n",
      "0.499999932821\n",
      "-3.528766632080078\n",
      "1.4142136573791504\n",
      "1.0\n",
      "-13.689520835876465\n",
      "53397\n",
      "['sheep', 'sheep', 'cattle', 'cattle', 'goats', 'goats', 'cows', 'cows', 'pigs', 'pigs', 'livestock', 'reindeer', 'swine', 'turkeys', 'poultry', 'goat', 'goat', 'goat', 'chickens', 'chickens', 'camels', 'cow', 'cow', 'deer', 'deer', 'yak', 'yak', 'donkeys', 'pig', 'pig', 'rabbits', 'rabbits', 'boar', 'boar', 'ponies', 'ponies', 'hogs', 'horses', 'horses', 'fowl', 'ferrets', 'calves', 'geese', 'elk', 'elk', 'bison', 'bison', 'crawfish', 'lambs', 'lambs', 'giraffe', 'goldfish', 'ostrich', 'mules', 'hippos', 'ox', 'ox', 'llama', 'llama', 'giraffes', 'boars', 'dogs', 'dogs', 'dogs', 'venison', 'shellfish', 'zebras', 'elephants', 'elephants', 'camel', 'camel', 'donkey', 'donkey', 'shrimp', 'shrimp', 'elephant', 'elephant', 'pandas', 'pandas', 'hamsters', 'horse', 'horse', 'horse', 'hog', 'hog', 'hog', 'rabbit', 'rabbit', 'chicken', 'chicken', 'chicken', 'lobsters', 'animals', 'animals', 'alligators', 'jellyfish', 'fish', 'fish', 'fish', 'tulips', 'crocodile', 'crocodile', 'parrots', 'puppy', 'puppy', 'raccoons', 'dogfish', 'dairy', 'dairy', 'pony', 'pony', 'lobster', 'lobster', 'cats', 'cats', 'cats', 'poodle', 'goose', 'goose', 'sardines', 'boobies', 'boobies', 'mule', 'vermin', 'puppies', 'puppies', 'brisket', 'catfish', 'oysters', 'piglet', 'piglet', 'roaches', 'chimps', 'herds', 'retriever', 'gorillas', 'cat', 'cat', 'cat', 'moose', 'moose', 'cacti', 'monkeys', 'monkeys', 'dog', 'dog', 'dog', 'zebra', 'zebra', 'chimpanzees', 'watermelons', 'tortoise', 'pasty', 'llamas', 'saplings', 'blueberries', 'koala', 'koala', 'locusts', 'terrier', 'terrier', 'flax', 'strawberries', 'cauliflower', 'lice', 'fleas', 'rats', 'rats', 'salmon', 'salmon', 'veal', 'pigeons', 'dachshund', 'toads', 'porcupine', 'turnips', 'blueberry', 'blueberry', 'caviar', 'caviar', 'raccoon', 'raccoon', 'squirrels', 'kittens', 'kittens', 'truffles', 'sloths', 'melons', 'maggots', 'stubble', 'stag', 'seaweed', 'asparagus', 'foxes', 'foxes', 'whale', 'whale', 'pineapples', 'pecan', 'narwhal']\n",
      "ugly\n",
      "beautify\n",
      "0. measure.n.01\n",
      "alternative names (lemmas): \"measure\", \"step\"\n",
      "hypErnyms:  [Synset('maneuver.n.04')]\n",
      "hypOrnyms:  [Synset('countermeasure.n.01'), Synset('precaution.n.01'), Synset('shark_repellent.n.01')]\n",
      "part_holonyms:  []\n",
      "part_meronyms:  []\n",
      "1. measure.n.02\n",
      "alternative names (lemmas): \"measure\", \"quantity\", \"amount\"\n",
      "hypErnyms:  [Synset('abstraction.n.06')]\n",
      "hypOrnyms:  [Synset('cordage.n.01'), Synset('definite_quantity.n.01'), Synset('fundamental_quantity.n.01'), Synset('indefinite_quantity.n.01'), Synset('magnetization.n.01'), Synset('octane_number.n.01'), Synset('playing_period.n.01'), Synset('point.n.06'), Synset('probability.n.01'), Synset('proof.n.03'), Synset('quantum.n.02'), Synset('radical.n.04'), Synset('relative_quantity.n.01'), Synset('system_of_measurement.n.01'), Synset('time_interval.n.01'), Synset('time_unit.n.01'), Synset('value.n.03'), Synset('volume.n.01'), Synset('volume.n.05')]\n",
      "part_holonyms:  []\n",
      "part_meronyms:  []\n",
      "2. bill.n.01\n",
      "alternative names (lemmas): \"bill\", \"measure\"\n",
      "hypErnyms:  [Synset('legal_document.n.01')]\n",
      "hypOrnyms:  [Synset('appropriation_bill.n.01'), Synset('bill_of_attainder.n.01'), Synset('bottle_bill.n.01'), Synset('farm_bill.n.01'), Synset('trade_bill.n.01')]\n",
      "part_holonyms:  []\n",
      "part_meronyms:  [Synset('rider.n.02')]\n",
      "3. measurement.n.01\n",
      "alternative names (lemmas): \"measurement\", \"measuring\", \"measure\", \"mensuration\"\n",
      "hypErnyms:  [Synset('activity.n.01')]\n",
      "hypOrnyms:  [Synset('actinometry.n.01'), Synset('algometry.n.01'), Synset('anemography.n.01'), Synset('anemometry.n.01'), Synset('angulation.n.01'), Synset('anthropometry.n.01'), Synset('arterial_blood_gases.n.01'), Synset('audiometry.n.02'), Synset('bathymetry.n.01'), Synset('calorimetry.n.01'), Synset('cephalometry.n.01'), Synset('densitometry.n.01'), Synset('dosimetry.n.01'), Synset('fetometry.n.01'), Synset('hydrometry.n.01'), Synset('hypsometry.n.01'), Synset('mental_measurement.n.01'), Synset('micrometry.n.01'), Synset('observation.n.01'), Synset('pelvimetry.n.01'), Synset('photometry.n.01'), Synset('quantification.n.02'), Synset('quantitative_analysis.n.01'), Synset('radioactive_dating.n.01'), Synset('reading.n.08'), Synset('sampling.n.03'), Synset('scaling.n.02'), Synset('seismography.n.01'), Synset('sound_ranging.n.01'), Synset('sounding.n.02'), Synset('spirometry.n.01'), Synset('surveying.n.01'), Synset('telemetry.n.01'), Synset('thermogravimetry.n.01'), Synset('thermometry.n.01'), Synset('tonometry.n.01'), Synset('viscometry.n.01')]\n",
      "part_holonyms:  []\n",
      "part_meronyms:  []\n",
      "4. standard.n.01\n",
      "alternative names (lemmas): \"standard\", \"criterion\", \"measure\", \"touchstone\"\n",
      "hypErnyms:  [Synset('system_of_measurement.n.01')]\n",
      "hypOrnyms:  [Synset('baseline.n.01'), Synset('benchmark.n.01'), Synset('earned_run_average.n.01'), Synset('gauge.n.02'), Synset('grade_point_average.n.01'), Synset('medium_of_exchange.n.01'), Synset('norm.n.01'), Synset('procrustean_standard.n.01'), Synset('scale.n.01'), Synset('yardstick.n.01')]\n",
      "part_holonyms:  []\n",
      "part_meronyms:  []\n",
      "5. meter.n.03\n",
      "alternative names (lemmas): \"meter\", \"metre\", \"measure\", \"beat\", \"cadence\"\n",
      "hypErnyms:  [Synset('poetic_rhythm.n.01')]\n",
      "hypOrnyms:  [Synset('catalexis.n.01'), Synset('common_measure.n.03'), Synset('metrical_foot.n.01'), Synset('scansion.n.01')]\n",
      "part_holonyms:  []\n",
      "part_meronyms:  []\n",
      "6. measure.n.07\n",
      "alternative names (lemmas): \"measure\", \"bar\"\n",
      "hypErnyms:  [Synset('musical_notation.n.01')]\n",
      "hypOrnyms:  []\n",
      "part_holonyms:  []\n",
      "part_meronyms:  []\n",
      "7. measuring_stick.n.01\n",
      "alternative names (lemmas): \"measuring_stick\", \"measure\", \"measuring_rod\"\n",
      "hypErnyms:  [Synset('measuring_instrument.n.01')]\n",
      "hypOrnyms:  [Synset('board_rule.n.01'), Synset('rule.n.12'), Synset('size_stick.n.01')]\n",
      "part_holonyms:  []\n",
      "part_meronyms:  []\n",
      "8. measure.n.09\n",
      "alternative names (lemmas): \"measure\"\n",
      "hypErnyms:  [Synset('container.n.01')]\n",
      "hypOrnyms:  [Synset('measuring_cup.n.01')]\n",
      "part_holonyms:  []\n",
      "part_meronyms:  []\n"
     ]
    }
   ],
   "source": [
    "def similarity(self, other):\n",
    "    self = nlp.vocab[self]\n",
    "    other = nlp.vocab[other]\n",
    "    if self.vector_norm == 0 or other.vector_norm == 0:\n",
    "        return 0.0\n",
    "    return numpy.dot(self.vector, other.vector) / (self.vector_norm * other.vector_norm)\n",
    "\n",
    "print(similarity('accumulate', 'gradually'))\n",
    "print(similarity('accumulate', 'slowly'))\n",
    "print(similarity('accumulate', 'steadily'))\n",
    "print(similarity('accumulate', 'rapidly'))\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def spacy_max_similarity(w1, w2):\n",
    "    word_1 = nlp.vocab[w1]\n",
    "    word_2 = nlp.vocab[w2]\n",
    "    return word_1.similarity(word_2)\n",
    "\n",
    "print(spacy_max_similarity('call', 'call'))\n",
    "\n",
    "import numpy as np\n",
    "apples = nlp.vocab[u\"the\"]\n",
    "print(apples.prob)\n",
    "print(apples.vector_norm)\n",
    "# prints 1.4142135381698608, or sqrt(2)\n",
    "print(np.sqrt(np.dot(apples.vector, apples.vector)))\n",
    "# prints 1.0\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def replace_with_word2vec2(word):\n",
    "    if type(word) != 'str':\n",
    "        word = str(word)\n",
    "    num_upbound = 200\n",
    "    word = nlp.vocab[word]\n",
    "    print(word.prob)\n",
    "    # Most similar ones\n",
    "    #queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    queries = [w for w in word.vocab if w.prob >= -15]\n",
    "    print(len(queries))\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    #print(by_similarity)\n",
    "    #print([w.lower_ for w in by_similarity[:num_upbound]])\n",
    "    cand_word_list = [w.lower_ for w in by_similarity[:num_upbound]] # some candidate words\n",
    "#     sim_list = [w for w in by_similarity[:num_upbound]]\n",
    "    #print(sim_list)\n",
    "    print(cand_word_list)\n",
    "    \n",
    "replace_with_word2vec2('Sheep')\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "class AntonymReplacer(object):\n",
    "    def replace(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "        for syn in wn.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def replace_negations(self, sent):\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i+1 < l:\n",
    "                ant = self.replace(sent[i+1])\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "        return words\n",
    "    \n",
    "replacer = AntonymReplacer()\n",
    "print(replacer.replace('beautiful'))\n",
    "print(replacer.replace('uglify'))\n",
    "\n",
    "#sent = [\"let's\", 'not', 'uglify', 'our', 'code']\n",
    "#replacer.replace_negations(sent)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "synsets = wn.synsets('measures', 'n')\n",
    "for i, syn in enumerate(synsets):\n",
    "\n",
    "    print('%d. %s' % (i, syn.name()))\n",
    "    print('alternative names (lemmas): \"%s\"' % '\", \"'.join(syn.lemma_names()))\n",
    "    print('hypErnyms: ', syn.hypernyms())\n",
    "    print('hypOrnyms: ', syn.hyponyms())\n",
    "    print('part_holonyms: ', syn.part_holonyms())\n",
    "    print('part_meronyms: ', syn.part_meronyms())\n",
    "#     print('test: ', syn.())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.602264404296875\n",
      "53397\n",
      "['measures', 'measure', 'measure', 'precautions', 'safeguards', 'compromises', 'curbs', 'initiatives', 'polices', 'indicators', 'remedies', 'strategies', 'measurements', 'maximizes', 'methods', 'methods', 'interventions', 'tradeoffs', 'bailouts', 'optimizations', 'workarounds', 'audits', 'minimizes', 'boosts', 'sanctions', 'metrics', 'thresholds', 'rationalizations', 'saps', 'inequalities', 'lags', 'actives', 'intangibles', 'minimums', 'mechanisms', 'offsets', 'legislation', 'reforms', 'discriminates', 'measurement', 'efficiencies', 'guarantees', 'hikes', 'restraints', 'ultrasounds', 'ordinances', 'methodologies', 'controls', 'controls', 'legalities', 'rebates', 'amplifies', 'policies', 'toggles', 'retards', 'calculates', 'tariffs', 'prohibitions', 'dosages', 'precaution', 'externalities', 'breakouts', 'increases', 'increases', 'realizations', 'laxatives', 'estimates', 'reductions', 'entitlements', 'benchmarks', 'tolerances', 'incentives', 'directives', 'factors', 'probiotics', 'cuts', 'cuts', 'amendments', 'measuring', 'inefficiencies', 'multipliers', 'assessments', 'decreases', 'tilts', 'tests', 'tests', 'wobbles', 'absolutes', 'contraceptives', 'adjusts', 'payouts', 'diminishes', 'degrades', 'punishments', 'techniques', 'techniques', 'steps', 'steps', 'approximations', 'bottlenecks', 'lessens', 'mitigation', 'quotas', 'mandates', 'regs', 'approaches', 'compressions', 'thrusts', 'nullifies', 'protections', 'enablers', 'meds', 'buffers', 'generalisations', 'approvals', 'tooltips', 'floaters', 'stances', 'filters', 'waypoints', 'loopholes', 'manipulations', 'reboots', 'laws', 'laws', 'enforces', 'shaders', 'indices', 'necessitates', 'warranties', 'analyses', 'overlays', 'furthers', 'overrides', 'proposals', 'burdens', 'hypotheticals', 'scopes', 'lowers', 'welds', 'ssris', 'deductions', 'enchantments', 'surveys', 'thingies', 'expenditures', 'hallucinogens', 'handouts', 'distorts', 'resistances', 'gauges', 'bounties', 'imbalances', 'evaluations', 'surcharge', 'skews', 'adjustments', 'regimes', 'stimulants', 'determines', 'customizations', 'efforts', 'processes', 'hexes', 'frowns', 'negates', 'antidepressants', 'handshakes', 'increments', 'statism', 'erections', 'dumbbells', 'presupposes', 'constants', 'deterrence', 'instants', 'leashes', 'inferences', 'vaccinations', 'protectionism', 'simulates', 'surges', 'confirmations', 'suppressors', 'bonuses', 'schemes', 'synergies', 'tracers', 'resets', 'blowouts', 'warmers', 'predicts', 'vibrates', 'procedures', 'comps', 'considerations', 'regulations', 'regulations', 'behaviours', 'swipes']\n"
     ]
    }
   ],
   "source": [
    "replace_with_word2vec2('measures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.390280723571777\n",
      "53397\n",
      "['beautifully', 'beautifully', 'brilliantly', 'meticulously', 'elegantly', 'delicately', 'powerfully', 'finely', 'expertly', 'wonderfully', 'masterfully', 'excellently', 'profusely', 'tastefully', 'crudely', 'decently', 'neatly', 'vividly', 'suitably', 'scantily', 'optimally', 'delightfully', 'weirdly', 'weirdly', 'flawlessly', 'conservatively', 'solidly', 'comically', 'stereotypically', 'intelligently', 'thoughtfully', 'impressively', 'maturely', 'humanely', 'liberally', 'conventionally', 'obsessively', 'acutely', 'abundantly', 'vocally', 'creatively', 'shamelessly', 'intimately', 'unevenly', 'intently', 'gloriously', 'effortlessly', 'brightly', 'pleasantly', 'eloquently', 'classically', 'thematically', 'awkwardly', 'painstakingly', 'diligently', 'graphically', 'rigorously', 'hideously', 'phonetically', 'ruthlessly', 'bizarrely', 'sensibly', 'obnoxiously', 'outrageously', 'justly', 'audibly', 'artistically', 'strangely', 'strangely', 'playfully', 'outwardly', 'competently', 'fervently', 'lovingly', 'faintly', 'irrationally', 'purposefully', 'transparently', 'mercilessly', 'subjectively', 'haphazardly', 'smartly', 'compulsively', 'incessantly', 'deliciously', 'coherently', 'faithfully', 'maniacally', 'shamefully', 'intensely', 'viciously', 'generously', 'rigidly', 'artificially', 'thinly', 'modestly', 'cleverly', 'spectacularly', 'physiologically', 'oddly', 'oddly', 'fantastically', 'rudely', 'uncomfortably', 'carelessly', 'furiously', 'densely', 'fiercely', 'cautiously', 'empirically', 'ideologically', 'discreetly', 'recklessly', 'uncontrollably', 'strikingly', 'statically', 'organically', 'gracefully', 'deceptively', 'habitually', 'lazily', 'chronically', 'aggressively', 'healthily', 'relentlessly', 'irresponsibly', 'visibly', 'wildly', 'disturbingly', 'enthusiastically', 'hilariously', 'crazily', 'painfully', 'subconsciously', 'horribly', 'proactively', 'tactically', 'aptly', 'embarrassingly', 'nicely', 'nicely', 'subtly', 'frustratingly', 'stylistically', 'identically', 'plausibly', 'doubly', 'defiantly', 'succinctly', 'bravely', 'bravely', 'inappropriately', 'securely', 'horrifically', 'horrendously', 'surgically', 'perpetually', 'defensively', 'pointlessly', 'monetarily', 'overtly', 'meaningfully', 'confidentially', 'gleefully', 'stupidly', 'amazingly', 'amazingly', 'incrementally', 'shockingly', 'eerily', 'ethically', 'philosophically', 'abnormally', 'instinctively', 'sparsely', 'seamlessly', 'mindlessly', 'needlessly', 'passively', 'psychologically', 'disproportionately', 'logistically', 'pathetically', 'competitively', 'laughably', 'astonishingly', 'blissfully', 'indiscriminately', 'earnestly', 'royally', 'unjustly', 'unambiguously', 'suspiciously', 'stunningly', 'anatomically', 'hysterically', 'passionately', 'purposely', 'annoyingly', 'monumentally']\n"
     ]
    }
   ],
   "source": [
    "replace_with_word2vec2('beautifully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = 'About 350 adventurers from nine countries set out on Saturday to climb , raft , bike and run in a 323-mile ( 517-km ) endurance race through the Canadian wilderness .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = nlp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About\n",
      "350\n",
      "adventurers\n",
      "from\n",
      "nine\n",
      "countries\n",
      "set\n",
      "out\n",
      "on\n",
      "Saturday\n",
      "to\n",
      "climb\n",
      ",\n",
      "raft\n",
      ",\n",
      "bike\n",
      "and\n",
      "run\n",
      "in\n",
      "a\n",
      "323-mile\n",
      "(\n",
      "517-km\n",
      ")\n",
      "endurance\n",
      "race\n",
      "through\n",
      "the\n",
      "Canadian\n",
      "wilderness\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in nlp:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About\n",
      "350\n",
      "adventurers\n",
      "from\n",
      "nine\n",
      "countries\n",
      "set\n",
      "out\n",
      "on\n",
      "Saturday\n",
      "to\n",
      "climb\n",
      ",\n",
      "raft\n",
      ",\n",
      "bike\n",
      "and\n",
      "run\n",
      "in\n",
      "a\n",
      "323-mile\n",
      "(\n",
      "517-km\n",
      ")\n",
      "endurance\n",
      "race\n",
      "through\n",
      "the\n",
      "Canadian\n",
      "wilderness\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in nlp:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = [('c', 2), ('a',1), ('c', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('c', 2), ('c', 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(temp, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.tokens.doc.Doc' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-804b1a38e955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"He is been here before.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'v'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;31m#print(np.text, np.root.text, np.root.dep_, np.root.head.text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object is not callable"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He is been here before.\")\n",
    "for token in doc:\n",
    "    print(token, len(list(token.subtree)), token.pos_, lemmatiser.lemmatize((str(token)), 'v'))\n",
    "    #print(np.text, np.root.text, np.root.dep_, np.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "s_cnt = 0\n",
    "for sent in raw_data:\n",
    "    cnt += 1\n",
    "    if list(sent)[-1] == '.':\n",
    "        s_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 'hello i am working on these tasks so please get out of here)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'i',\n",
       " 'am',\n",
       " 'working',\n",
       " 'on',\n",
       " 'these',\n",
       " 'tasks',\n",
       " 'so',\n",
       " 'please',\n",
       " 'get',\n",
       " 'out',\n",
       " 'of',\n",
       " 'here)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " 'o',\n",
       " ' ',\n",
       " 'i',\n",
       " ' ',\n",
       " 'a',\n",
       " 'm',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'a',\n",
       " 's',\n",
       " 'k',\n",
       " 's',\n",
       " ' ',\n",
       " 's',\n",
       " 'o',\n",
       " ' ',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " 'a',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'g',\n",
       " 'e',\n",
       " 't',\n",
       " ' ',\n",
       " 'o',\n",
       " 'u',\n",
       " 't',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ')']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ['a b', 'b b', 'c b ']\n",
    "b = ['eb ', 'f bb', 'g b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b', 'b b', 'c b ', 'eb ', 'f bb', 'g b']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 어떤 것을 교체할 것인가?\n",
    "# 전략1: 모두 교체\n",
    "# 전략2: 전치사(closed-class words) 빼고 다 교체\n",
    "# 전략3: 명사, 동사만 교체\n",
    "# 전략4: dependency가 높은 명사, 동사만 교체\n",
    "# 엔티티의 head\n",
    "# 전략5: root token만 교체\n",
    "\n",
    "### 어떻게 교체할 것인가?\n",
    "# word2vec을 사용해서\n",
    "# wordnet을 사용해서\n",
    "# sense2vec 사용하면 임베딩에서 태그정보도 같이 넣을 수 있음.\n",
    "\n",
    "# NER사전을 통해 다른 엔티티로 교체 (NER에 한해서만) - https://explosion.ai/blog/sense2vec-with-spacy\n",
    "## -> 그러면 엔티티에 대한 패턴을 좀 더 확장할 수 있지 않을까\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
